# -*- coding: utf-8 -*-
import traceback
import wandb

"""KD_Haitongli_Pascal(3).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1M4QgfiQpn7Vf7ekbyZeQvmZOZcIvxxya
"""



#from google.colab import drive
#drive.mount('/content/drive')
#!git clone https://github.com/keshik6/pascal-voc-classification.git
#%cd pascal-voc-classification
#%cd src

from torch.utils.tensorboard import SummaryWriter
import torch
from torch.nn import init
import torch.nn as nn
from torchvision import transforms
import torchvision.models as  models
from torch.utils.data import DataLoader
#from dataset import PascalVOC_Dataset
import torch.optim as optim
#from train import train_model, test
#from utils import encode_labels, plot_history
import os
import torch.utils.model_zoo as model_zoo
#import utils
import matplotlib.pyplot as plt
from tqdm import tqdm
import gc
from sklearn.metrics import average_precision_score, accuracy_score
import os
#from utils import get_ap_score
import random
import numpy as np
import json
from torch.optim.lr_scheduler import StepLR
from torchvision import models
import torch.nn.functional as F
from torch.autograd import Variable
import logging
from torch.autograd import Variable
import shutil
from collections import OrderedDict

from PIL import Image
from matplotlib.pyplot import imshow
from torchvision import models, transforms
from torch.autograd import Variable
from torch.nn import functional as F
from torch import topk
import numpy as np
import skimage.transform

import tensorflow as tf
import scipy.misc
from io import BytesIO         # Python 3.x
from torch.utils.tensorboard import SummaryWriter


import torchvision.transforms.functional as VF

os.environ["CUDA_VISIBLE_DEVICES"] = "1"

use_cuda = torch.cuda.is_available()
device = torch.device("cuda" if use_cuda else "cpu")

print("Available device = ", device)

#from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad
#from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget
#from pytorch_grad_cam.utils.image import show_cam_on_image

#class to take a dictionary and modify it so that you can access items in it like like  dict.item instead of dict['item']
class DotDict(dict):
    def __getattr__(self, attr):
        try:
            return self[attr]
        except KeyError:
            raise AttributeError(attr)

def applyThreshold(matrix, threshold):
  # "#print("heatmap matrix: "+str(matrix))
   #print("length "+str(len(matrix)))
   #print("type "+str(matrix.dtype))
   matrix[matrix < threshold] = 0
   matrix[matrix >= threshold] = 1
   return matrix

def loss_fn(outputs, labels):
    """
    Compute the cross entropy loss given outputs and labels.
    Args:
        outputs: (Variable) dimension batch_size x 6 - output of the model
        labels: (Variable) dimension batch_size, where each element is a value in [0, 1, 2, 3, 4, 5]
    Returns:
        loss (Variable): cross entropy loss for all images in the batch
    Note: you may use a standard loss function from http://pytorch.org/docs/master/nn.html#loss-functions. This example
          demonstrates how you can easily define a custom loss function.
    """
    return nn.CrossEntropyLoss()(outputs, labels)

def compare_sizes(studmat, teachmat):
    size1 = studmat.size()
    size2 = teachmat.size()

    if size1 > size2:
        print("WARNING!!! Student heatmap is bigger than the teacher heatmap ")
        return studmat
    elif size1 < size2:
        return teachmat
    else:
        return teachmat


# Function to upsample a single matrix from 4x4 to 8x8
def upsample_matrix(studmat, teachmat):
    biggermat = compare_sizes(studmat, teachmat)
  
    if not isinstance(studmat, torch.Tensor):
        print("DANGER: Matrix to upsample is not a tensor")
    upsampled_matrix = F.interpolate(studmat.unsqueeze(0).unsqueeze(0), size=(biggermat.shape[0], biggermat.shape[0]), mode='nearest').squeeze()
    if not isinstance(upsampled_matrix, torch.Tensor):
        print("ERROR: Upsampled matrix is not a tensor")    
    if upsampled_matrix.size() != biggermat.size():
        print("ERROR: upsampled matrix is not the same size as the bigger matrix out of the 2 to compare")    
   # return torch.kron(matrix, torch.ones((2, 2)))
    return upsampled_matrix

def loss_fn_kd_noHeatMap(outputs, labels, teacher_outputs, params):
    alpha = params.alpha
    T = params.temperature
   # print(type(teacher_outputs))
    #print(type(outputs))
    KD_loss = nn.KLDivLoss(reduction = "batchmean")(F.log_softmax(outputs/T, dim=1),
                             F.softmax(teacher_outputs/T, dim=1)) * (alpha * T * T) + \
              F.cross_entropy(outputs, labels) * (1. - alpha)

    return KD_loss


def check_nan(array):
   if isinstance(array, np.ndarray):   
    if np.isnan(array).any():
      print("Error, NaN detected")
      return True
    else:
      return False

   elif isinstance(array, torch.Tensor):
    if torch.isnan(array).any():
      print("Error, NaN detected")
      return True
    else:
      return False
   else:
   # print("array: "+str(array))
  #  print("array type: "+str(array.dtype))
    print("array is most likely a list and not a numpy array or a pytorch tensor, ask for help on how to fix this")
    raise ValueError("Unsupported data type. Only numpy arrays and PyTorch tensors are supported.")
    
# Example usage:
# Create a tensor with NaN value
#tensor_with_nan = torch.tensor([1.0, 2.0, float('nan'), 4.0])

# Call the function to check for NaN
#check_nan(tensor_with_nan)

def MSEloss(inputs, gt):
    c = nn.MSELoss()
    loss = c(inputs,gt)
    return loss

def loss_fn_kd_thresh(outputs, labels, teacher_outputs, params, student_heatmap_batch, teacher_heatmap_batch, threshold):
    """
    Compute the knowledge-distillation (KD) loss given outputs, labels.
    "Hyperparameters": temperature and alpha
    NOTE: the KL Divergence for PyTorch comparing the softmaxs of teacher
    and student expects the input tensor to be log probabilities! See Issue #2
    """
    if not isinstance(student_heatmap_batch, torch.Tensor):
        print("DANGER: student heatmap batch is NOT of type Tensor ")
        
    if not isinstance(teacher_heatmap_batch, torch.Tensor):
        print("DANGER: teacher heatmap batch is NOT of type Tensor ")    
    alpha = params.alpha
    T = params.temperature
    KD_loss = nn.KLDivLoss(reduction = "batchmean")(F.log_softmax(outputs/T, dim=1),
                             F.softmax(teacher_outputs/T, dim=1)) * (alpha * T * T) + \
              F.cross_entropy(outputs, labels) * (1. - alpha)

    
    
    if (check_nan(student_heatmap_batch)):
     print("NaN detected on student heatmap batch before applying threshold")
    #print()
    if (check_nan(teacher_heatmap_batch)):
     print("NaN detected on teacher heatmap batch before applying threshold")
    student_heatmap_batch = applyThreshold(student_heatmap_batch, threshold)
    if (check_nan(student_heatmap_batch)):
     print("NaN detected on student heatmap batch AFTER applying threshold")
    teacher_heatmap_batch = applyThreshold(teacher_heatmap_batch, threshold)
    if (check_nan(teacher_heatmap_batch)):
      print("NaN detected on teacher heatmap batch AFTER applying threshold")
#print("stud heatmap  batch shape "+str(student_heatmap_batch.shape))
    #print("teacher heatmap batch shape: "+str(teacher_heatmap_batch.shape))
    #print("teach heatmap  batch shape "+str(teacher_heatmap_batch.shape))
    # Upsample each matrix in the batch
    #print("teacher heatmap batch shape "+str(teacher_heatmap_batch.shape))
    #print("student heatmap batch shape "+str(student_heatmap_batch.shape))
    teachsample = teacher_heatmap_batch[0]
    #print("teacher sample shape " +str(teachsample.shape))
    studsample = student_heatmap_batch[0]
    #print("student sample shape "+str(studsample.shape))
    student_heatmap_upsampled_batch = torch.empty((len(student_heatmap_batch), teachsample.shape[0], teachsample.shape[0]), dtype=student_heatmap_batch.dtype)
   # student_heatmap_upsampled_batch = student_heatmap_batch.clone().reshape((len(student_heatmap_batch), teachsample.shape[0], teachsample.shape[0]))
    for i, matrix in enumerate(student_heatmap_batch):
        student_heatmap_upsampled_batch[i] = upsample_matrix(matrix, teachsample)
   # student_heatmap_batch = np.array([upsample_matrix(matrix, teachsample) for matrix in student_heatmap_batch])
    
    heatmap_dist_batch = torch.abs(student_heatmap_upsampled_batch - teacher_heatmap_batch)
  
    if not isinstance(heatmap_dist_batch, torch.Tensor):
        print("DANGER: heatmap_dist batch is NOT of type Tensor ")
    if (check_nan(student_heatmap_upsampled_batch)):
        print("NaN detected on student heatmap batch AFTER upsampling")
    #print("stud heatmap  batch shape "+str(student_heatmap_batch.shape))
        
    if (check_nan(heatmap_dist_batch)):
      print("NaN detected on heatmap distance before sum")
 #   print("student heatmap batch shape: "+str(student_heatmap_batch.shape))
  #  print("teacher heatmap batch shape: "+str(teacher_heatmap_batch.shape))
   # print("shape of abs(studentheatmapbatch - teacherheatmapbatch): "+str(heatmap_dist_batch.shape))
    heatmap_dist =  torch.sum(heatmap_dist_batch,axis=(1,2))
    if not isinstance(heatmap_dist, torch.Tensor):
        print("DANGER: Heatmap dissimilarity before averaging is NOT of type Tensor ")
   # print("shape of sum of heatmap_dist (what we just did in the step before with the abs): "+str(heatmap_dist.shape))
    avg_heatmap_dist = torch.mean(heatmap_dist)
    if not isinstance(avg_heatmap_dist, torch.Tensor):
        print("DANGER: avg_heatmap_dist is NOT of type Tensor ")
    #print("avg heatmap dist:" +str(avg_heatmap_dist))
    return KD_loss, avg_heatmap_dist


def loss_fn_kd_nothresh(outputs, labels, teacher_outputs, params, student_heatmap_batch, teacher_heatmap_batch

               ):
    """
    Compute the knowledge-distillation (KD) loss given outputs, labels.
    "Hyperparameters": temperature and alpha
    NOTE: the KL Divergence for PyTorch comparing the softmaxs of teacher
    and student expects the input tensor to be log probabilities! See Issue #2
    """
    if not isinstance(student_heatmap_batch, torch.Tensor):
        print("DANGER: student heatmap batch is NOT of type Tensor ")
        
    if not isinstance(teacher_heatmap_batch, torch.Tensor):
        print("DANGER: teacher heatmap batch is NOT of type Tensor ")  
    alpha = params.alpha
    T = params.temperature
    KD_loss = nn.KLDivLoss(reduction = "batchmean")(F.log_softmax(outputs/T, dim=1),
                             F.softmax(teacher_outputs/T, dim=1)) * (alpha * T * T) + \
              F.cross_entropy(outputs, labels) * (1. - alpha)

    
    
    if (check_nan(student_heatmap_batch)):
     print("NaN detected on student heatmap batch (this is a no threshold experiment)")

#print("stud heatmap  batch shape "+str(student_heatmap_batch.shape))

    #print("teach heatmap  batch shape "+str(teacher_heatmap_batch.shape))
    # Upsample each matrix in the batch
   # print("teacher heatmap batch shape "+str(teacher_heatmap_batch.shape))
    #print("student heatmap batch shape "+str(student_heatmap_batch.shape))
    teachsample = teacher_heatmap_batch[0]
    #print("teacher sample shape " +str(teachsample.shape))
    studsample = student_heatmap_batch[0]
    #print("student sample shape "+str(studsample.shape))
    student_heatmap_upsampled_batch = torch.empty((len(student_heatmap_batch), teachsample.shape[0], teachsample.shape[0]), dtype=student_heatmap_batch.dtype)
 
    #student_heatmap_upsampled_batch = student_heatmap_batch.clone().reshape((len(student_heatmap_batch), teachsample.shape[0], teachsample.shape[0]))
    for i, matrix in enumerate(student_heatmap_batch):
        student_heatmap_upsampled_batch[i] = upsample_matrix(matrix, teachsample)
   
    #print("student_heatmap upsampled batch shape "+str(student_heatmap_upsampled_batch.shape))
    #print("teacher heatmap_batch shape "+str(teacher_heatmap_batch.shape))
    heatmap_MSE = MSEloss(student_heatmap_upsampled_batch, teacher_heatmap_batch)
    
    #print("heatmap MSE "+str(heatmap_MSE))
    #student_heatmap_batch = np.array([upsample_matrix(matrix) for matrix in student_heatmap_batch])
    if (check_nan(student_heatmap_upsampled_batch)):
        print("NaN detected on student heatmap batch AFTER upsampling")
    #print("stud heatmap  batch shape "+str(student_heatmap_batch.shape))
    heatmap_dist_batch = torch.abs(student_heatmap_upsampled_batch - teacher_heatmap_batch)
    if (check_nan(heatmap_dist_batch)):
      print("NaN detected on heatmap distance before sum")
    heatmap_dist =  torch.sum(heatmap_dist_batch,axis=(1,2))
    if (check_nan(heatmap_dist)):
      print("Nan detected on heatmap distance AFTER sum but before average")
    avg_heatmap_dist = torch.mean(heatmap_dist)
    #print("average heamtap dist "+str(avg_heatmap_dist))
    return KD_loss, heatmap_MSE





def accuracy(outputs, labels):
    """
    Compute the accuracy, given the outputs and labels for all images.
    Args:
        outputs: (np.ndarray) output of the model
        labels: (np.ndarray) [0, 1, ..., num_classes-1]
    Returns: (float) accuracy in [0,1]
    """
    outputs = np.argmax(outputs, axis=1)
    return np.sum(outputs==labels)/float(labels.size)


metrics = {
    'accuracy': accuracy
    # could add more metrics such as accuracy for each token type
}

def plot_history(train_hist, val_hist, y_label, filename, labels=["train", "validation"]):
    """
    Plot training and validation history

    Args:
        train_hist: numpy array consisting of train history values (loss/ accuracy metrics)
        valid_hist: numpy array consisting of validation history values (loss/ accuracy metrics)
        y_label: label for y_axis
        filename: filename to store the resulting plot
        labels: legend for the plot

    Returns:
        None
    """
    # Plot loss and accuracy
    xi = [i for i in range(0, len(train_hist), 2)]
    plt.plot(train_hist, label = labels[0])
    plt.plot(val_hist, label = labels[1])
    plt.xticks(xi)
    plt.legend()
    plt.xlabel("Epoch")
    plt.ylabel(y_label)
    plt.savefig(filename)
    plt.show()

def train(model, optimizer, loss_fn, dataloader, metrics, params, wandbrun):
    """Train the model on `num_steps` batches
    Args:
        model: (torch.nn.Module) the neural network
        optimizer: (torch.optim) optimizer for parameters of model
        loss_fn:
        dataloader:
        metrics: (dict)
        params: (Params) hyperparameters
    """

    # set model to training mode
    model.train()

    # summary for current training loop and a running average object for loss
    summ = []
    loss_avg = RunningAverage()

    # Use tqdm for progress bar
    with tqdm(total=len(dataloader)) as t:
        for i, (train_batch, labels_batch) in enumerate(dataloader):
            # move to GPU if available
            if params.cuda:
                train_batch, labels_batch = train_batch.cuda(), \
                                            labels_batch.cuda()
            # convert to torch Variables
            train_batch, labels_batch = Variable(train_batch), Variable(labels_batch)

            # compute model output and loss
            output_batch = model(train_batch)
            loss = loss_fn(output_batch, labels_batch)

            # clear previous gradients, compute gradients of all variables wrt loss
            optimizer.zero_grad()
            loss.backward()

            # performs updates using calculated gradients
            optimizer.step()

            # Evaluate summaries only once in a while
            if i % params.save_summary_steps == 0:
                # extract data from torch Variable, move to cpu, convert to numpy arrays
                output_batch = output_batch.data.cpu().numpy()
                labels_batch = labels_batch.data.cpu().numpy()

                # compute all metrics on this batch
                summary_batch = {metric:metrics[metric](output_batch, labels_batch)
                                 for metric in metrics}
                summary_batch['loss'] = loss.data
                summ.append(summary_batch)

            # update the average loss
            loss_avg.update(loss.data)

            t.set_postfix(loss='{:05.3f}'.format(loss_avg()))
            t.update()
    if wandbrun == True:
        wandb.log({"train_Loss": float(loss_avg())})

    # compute mean of all metrics in summary
    metrics_mean = {
    metric: np.mean(
        [x[metric].cpu().numpy() if isinstance(x[metric], torch.Tensor) else x[metric] for x in summ]
    )
    for metric in summ[0]
    }
    
    
    
    metrics_string = " ; ".join("{}: {:05.3f}".format(k, v) for k, v in metrics_mean.items())
    logging.info("- Train metrics: " + metrics_string)

def train_and_evaluate(model, train_dataloader, val_dataloader, optimizer,
                  #     loss_fn, metrics, params, model_dir, model_arch, restore_file=None, wandbrun = False):
                  loss_fn, metrics, params, model_arch, restore_file=None, wandbrun = False):
    """Train the model and evaluate every epoch.
    Args:
        model: (torch.nn.Module) the neural network
        params: (Params) hyperparameters
        model_dir: (string) directory containing config, weights and log
        restore_file: (string) - name of file to restore from (without its extension .pth.tar)
    """
    # reload weights from restore_file if specified
    if restore_file is not None:
        restore_path = os.path.join(model_dir, restore_file + '.pth.tar')
        logging.info("Restoring parameters from {}".format(restore_path))
        load_checkpoint(restore_path, model, optimizer)

    best_val_acc = 0.0

    # learning rate schedulers for different models:
    if params.model_version == "resnet18":
        scheduler = StepLR(optimizer, step_size=30, gamma=0.1)
    # for cnn models, num_epoch is always < 100, so it's intentionally not using scheduler here
    elif params.model_version == "cnn":
        scheduler = StepLR(optimizer, step_size=30, gamma=0.2)
    
    for epoch in range(params.num_epochs):
        print("-------Epoch {}----------".format(epoch+1))
        scheduler.step()

        # Run one epoch
        logging.info("Epoch {}/{}".format(epoch + 1, params.num_epochs))

        # compute number of batches in one epoch (one full pass over the training set)
        train(model, optimizer, loss_fn, train_dataloader, metrics, params,wandbrun)

        # Evaluate for one epoch on validation set
        val_metrics = evaluate(model, loss_fn, val_dataloader, metrics, params)
        
        
        
        val_loss = val_metrics['loss']
           

        val_acc = val_metrics['accuracy']
        is_best = val_acc>=best_val_acc
        if wandbrun == True:

            wandb.log({"val_acc": val_acc, "epoch": epoch+1})       
            wandb.log({"val_loss": val_loss, "epoch": epoch+1})    
                
        settings_dict ={
            "student_model": model_arch,
            "useKD": False,
            "usethreshold": False
        }
        path_base = "/bin/smartinez/kdfiles/NoKD"
        destination_folder = f'{path_base}/{model_arch}' 
        # Save weights
        save_checkpoint({'epoch': epoch + 1,
                               'state_dict': model.state_dict(),
                               'optim_dict' : optimizer.state_dict()},
                               is_best=is_best,
                               checkpoint=destination_folder)

        # If best_eval, best_save_path
        if is_best:
            logging.info("- Found new best accuracy")
            print("Old best val accuracy: "+str(best_val_acc))
            best_val_acc = val_acc
            print("New best val accuracy: "+str(best_val_acc))
            if wandbrun == True:

           
             wandb.log({"best_val_acc": best_val_acc, "epoch": epoch+1})     
            # Save best val metrics in a json file in the model directory
            best_json_path = os.path.join(model_dir, "metrics_val_best_weights.json")
            save_dict_to_json(val_metrics, best_json_path)

        # Save latest val metrics in a json file in the model directory
        last_json_path = os.path.join(model_dir, "metrics_val_last_weights.json")
        save_dict_to_json(val_metrics, last_json_path)

"""Evaluate"""

def evaluate(model, loss_fn, dataloader, metrics, params):
    """Evaluate the model on `num_steps` batches.
    Args:
        model: (torch.nn.Module) the neural network
        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch
        dataloader: (DataLoader) a torch.utils.data.DataLoader object that fetches data
        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch
        params: (Params) hyperparameters
        num_steps: (int) number of batches to train on, each of size params.batch_size
    """

    # set model to evaluation mode
    model.eval()

    # summary for current eval loop
    summ = []

    m = torch.nn.Sigmoid()
   # running_loss = 0


    # compute metrics over the dataset
    with torch.no_grad():

      for data_batch, labels_batch in tqdm(dataloader):

          # move to GPU if available
          if params.cuda:
              data_batch, labels_batch = data_batch.cuda(), labels_batch.cuda()
          # fetch the next evaluation batch
          data_batch, labels_batch = Variable(data_batch), Variable(labels_batch)

          # compute model output
          output_batch = model(data_batch)
          loss = loss_fn(output_batch, labels_batch)


          #running_loss += loss.item()


          # extract data from torch Variable, move to cpu, convert to numpy arrays
          output_batch = output_batch.data.cpu().numpy()
          labels_batch = labels_batch.data.cpu().numpy()

          # compute all metrics on this batch
          summary_batch = {metric: metrics[metric](output_batch, labels_batch)
                          for metric in metrics}
          #summary_batch['loss'] = loss.data[0]
          summary_batch['loss']=loss

          summ.append(summary_batch)

         # del data_batch, labels_batch, output_batch
         # gc.collect()
          #torch.cuda.empty_cache()


      # compute mean of all metrics in summary

    #metrics_mean = {metric:torch.mean(torch.as_tensor([x[metric] for x in summ])) for metric in summ[0]}
    #metrics_mean = {
    #metric: torch.mean(torch.from_numpy(np.array([x[metric] for x in summ])))
    #for metric in summ[0]
    #    }
    metrics_mean = {
    metric: np.mean(
        [x[metric].cpu().numpy() if isinstance(x[metric], torch.Tensor) else x[metric] for x in summ]
    )
    for metric in summ[0]
    }
    
    
    metrics_string = " ; ".join("{}: {:05.3f}".format(k, v) for k, v in metrics_mean.items())
    logging.info("- Eval metrics : " + metrics_string)



    num_samples = float(len(dataloader.dataset))
    #avg_test_loss = running_loss/num_samples

    #print('test_loss (Cross entropy loss): {:.4f}, test_avg_precision(not the same metric as accuracy):{:.3f}'.format(
     #                 avg_test_loss, test_map))
    #print('test_loss (Cross entropy loss): {:.4f}'.format(            avg_test_loss))

    #print("num_samples: "+str(num_samples))
    #print("summary length: "+str(len(summ)))
    #print("loss without averaging: "+str(running_loss))
    #print("precision without averaging: "+str(running_ap))
    #print("metrics_mean: "+str(metrics_mean))



    return metrics_mean

"""Train KD"""

def train_kd_vanilla(model, teacher_model, optimizer, loss_fn_kd_noHeatMap, dataloader, metrics, params):


    # set model to training mode
    model.train()
    teacher_model.eval()

    # summary for current training loop and a running average object for loss
    summ = []
    loss_avg = RunningAverage()
    heatmap_dissimilarity_avg = RunningAverage()

   # running_loss = 0.0
   # running_ap = 0.0
    m = torch.nn.Sigmoid()

    tr_loss, tr_map = [], []
    with tqdm(total=len(dataloader)) as t:
      for i, (train_batch, labels_batch) in enumerate(dataloader):
        #  print("size of train batch "+str(train_batch.shape))
          # move to GPU if available
          if params.cuda:
              train_batch, labels_batch = train_batch.cuda(), \
                                          labels_batch.cuda()
          # convert to torch Variables
          # train_batch, labels_batch = Variable(train_batch), Variable(labels_batch)
          #print("sie of train batch "+str(train_batch.shape))
          # compute model output, fetch teacher output, and compute KD loss
          #output_batch = model(train_batch)

          # get one batch output from teacher_outputs list
          #student_output_batch = model(train_batch)


          train_batch, labels_batch = Variable(train_batch), Variable(labels_batch)
          with torch.no_grad():
            teacher_output_batch = teacher_model(train_batch)
            #print("teacher output batch type after using no grad without going into the cuda part "+str(type(teacher_output_batch)))
            if params.cuda:
              teacher_output_batch = teacher_output_batch.cuda()
              # print("teacher output batch type after using no grad AFTER going into the cuda part "+str(type(teacher_output_batch)))


          student_output_batch = model(train_batch)


          loss = loss_fn_kd_noHeatMap(outputs=student_output_batch, labels=labels_batch, teacher_outputs=teacher_output_batch, params=params)

          # clear previous gradients, compute gradients of all variables wrt loss
          #print(type(loss))
          optimizer.zero_grad()
          loss.backward()

          # performs updates using calculated gradients
          optimizer.step()

          # Evaluate summaries only once in a while
          if i % params.save_summary_steps == 0:
              # extract data from torch Variable, move to cpu, convert to numpy arrays
              student_output_batch = student_output_batch.data.cpu().numpy()
              labels_batch = labels_batch.data.cpu().numpy()

              # compute all metrics on this batch
              summary_batch = {metric:metrics[metric](student_output_batch, labels_batch)
                                for metric in metrics}
              summary_batch['loss'] = loss.data#it was loss.data[0]
              summ.append(summary_batch)

          # update the average loss
          loss_avg.update(loss.data)#it was loss.data[0]


          t.set_postfix(loss='{:05.3f}'.format(loss_avg()))
          t.update()

    #      del train_batch, labels_batch, student_output_batch
     #     gc.collect()
      #    torch.cuda.empty_cache()



    # compute mean of all metrics in summary
   # tb.add_scalar('KLD average loss', loss, epoch)
    #tb.add_scalar('Heatmap loss', heatmap_dissimilarity, epoch)
    #metrics_mean = {metric:torch.mean(torch.as_tensor([x[metric] for x in summ])) for metric in summ[0]}
    metrics_mean = {
    metric: np.mean(
        [x[metric].cpu().numpy() if isinstance(x[metric], torch.Tensor) else x[metric] for x in summ]
    )
    for metric in summ[0]
    }
    
    
    metrics_string = " ; ".join("{}: {:05.3f}".format(k, v) for k, v in metrics_mean.items())
    logging.info("- Train metrics: " + metrics_string)
    #num_samples = float(len(train_loader.dataset))
    #tr_loss_ = running_loss/num_samples
    #tr_map_ = running_ap/num_samples
    return loss_avg()


# Defining train_kd & train_and_evaluate_kd functions
def train_kd_thresh(model, teacher_model, optimizer, loss_fn_kd_thresh, dataloader, metrics, params, experiment
                    , student_activated_features, teacher_activated_features, threshold, KLDgamma, wandbrun):
    """Train the model on `num_steps` batches
    Args:
        model: (torch.nn.Module) the neural network
        optimizer: (torch.optim) optimizer for parameters of model
        loss_fn_kd:
        dataloader:
        metrics: (dict)
        params: (Params) hyperparameters
    """
   # print( "now we're inside train_kd_thresh")
    # set model to training mode
    model.train()
    teacher_model.eval()

    # summary for current training loop and a running average object for loss
    summ = []
    combinedLoss_avg = RunningAverage()
    heatmap_dissimilarity_avg = RunningAverage()
    kl_loss_avg = RunningAverage()

   # running_loss = 0.0
   # running_ap = 0.0
    #m = torch.nn.Sigmoid()

 #   tr_loss, tr_map = [], []
  #  val_loss, val_map = [], []
   # best_val_map = 0.0
    student_model = model
    #print("about to set the weight softmax params ")
    student_weight_softmax_params =list(student_model.linear.parameters()) # This gives a list of weights for the fully connected layers
    student_weight_softmax = student_weight_softmax_params[0].data
    student_weight_softmax.requires_grad = True
    teacher_weight_softmax_params = list(teacher_model.module.classifier.parameters()) # This gives a list of weights for the fully connected layers
    teacher_weight_softmax = teacher_weight_softmax_params[0].data
    teacher_weight_softmax.requires_grad = True
    #print("weight softmax params set")
    # Use tqdm for progress bar
    with tqdm(total=len(dataloader)) as t:
      #print("we didn't crash with line with tqdm(total=len(dataloader)) as t: ")  
      for i, (train_batch, labels_batch) in enumerate(dataloader):
         # print("inside batch loop")
        #  print("size of train batch "+str(train_batch.shape))
          # move to GPU if available
          if params.cuda:
              train_batch, labels_batch = train_batch.cuda(), \
                                          labels_batch.cuda()
          # convert to torch Variables
          # train_batch, labels_batch = Variable(train_batch), Variable(labels_batch)
          #print("sie of train batch "+str(train_batch.shape))
          # compute model output, fetch teacher output, and compute KD loss
          #output_batch = model(train_batch)

          # get one batch output from teacher_outputs list
          #student_output_batch = model(train_batch)
          #if experiment != 'Noheatmap_dissimilarity':
         # print("now let's try to turn the batches into variables")
          train_batch, labels_batch = Variable(train_batch, requires_grad= True), Variable(labels_batch)
          #print("now let's see if we can get the teacher output")
          teacher_output_batch = teacher_model(train_batch)
          if params.cuda:
            teacher_output_batch = teacher_output_batch.cuda()

          #else:
           # train_batch, labels_batch = Variable(train_batch), Variable(labels_batch)
           # with torch.no_grad():
            #  teacher_output_batch = teacher_model(train_batch)
              #print("teacher output batch type after using no grad without going into the cuda part "+str(type(teacher_output_batch)))
             # if params.cuda:
              #  teacher_output_batch = teacher_output_batch.cuda()
               # print("teacher output batch type after using no grad AFTER going into the cuda part "+str(type(teacher_output_batch)))

          #print("now trying to get the output from the student")
          student_output_batch = model(train_batch)


          if experiment == 'TrueClass':
            #print("using true classes")
            #print("student activated features.features"+str(student_activated_features.features))
            if (print_if_all_zero(student_activated_features.features)):
              print("student_activated_features.features is all 0s")
             # print("student_activated_features.features: "+str(student_activated_features.features))


            student_heatmap_batch = getCAMBatch(student_activated_features.features, student_weight_softmax, labels_batch)
           # print("got student heatmap batch")
            teacher_heatmap_batch = getCAMBatch(teacher_activated_features.features, teacher_weight_softmax, labels_batch)
          #  print( "got teacher heatmap batch, now we need kl loss and heatmap dissimilarity")
            kl_loss, heatmap_dissimilarity = loss_fn_kd_thresh(student_output_batch, labels_batch, teacher_output_batch, params, student_heatmap_batch, teacher_heatmap_batch, threshold)

          if experiment == 'TopClass':
              
            #print("using top class of teacher")
            teacher_preds = torch.sigmoid(teacher_output_batch)
           # print("got teacher preds")
            teacher_pred_probs = F.softmax(teacher_preds, dim=1).data.squeeze()
            max_probs, max_indices = torch.max(teacher_pred_probs, dim=1)
            student_heatmap_batch = getCAMBatch(student_activated_features.features, student_weight_softmax, max_indices)
          #  print("student activated features.features"+str(student_activated_features.features))
           # print("got student heatmap batch")
            if (check_nan(student_heatmap_batch)):
              print("found a NaN on the student heatmap batch")
            teacher_heatmap_batch = getCAMBatch(teacher_activated_features.features, teacher_weight_softmax, max_indices)
            #print( "got teacher heatmap batch, now we need kl loss and heatmap dissimilarity")
            kl_loss, heatmap_dissimilarity = loss_fn_kd_thresh(student_output_batch, labels_batch, teacher_output_batch, params, student_heatmap_batch, teacher_heatmap_batch, threshold)


          if experiment == 'AllClasses':
            #print("using all classes")
            #print("student activated features.features"+str(student_activated_features.features))
            student_heatmap_batch = classCAMSbatch(student_activated_features.features, student_weight_softmax)
            #print("got student heatmap batch")
            #print("student activated features.features"+str(student_activated_features.features))
            if (check_nan(student_heatmap_batch)):
              print("found a NaN on the student heatmap batch")
          
            teacher_heatmap_batch = classCAMSbatch(teacher_activated_features.features, teacher_weight_softmax)
            if (check_nan(teacher_heatmap_batch)):
              print("found a NaN on the teacher heatmap batch")
          #  print( "got teacher heatmap batch, now we need kl loss and heatmap dissimilarity")
            kl_loss, heatmap_dissimilarity = loss_fn_kd_thresh(student_output_batch, labels_batch, teacher_output_batch, params, student_heatmap_batch, teacher_heatmap_batch, threshold)

          #with torch.no_grad():
          #    output_teacher_batch = teacher_model(train_batch)
          #if params.cuda:
           #   output_teacher_batch = output_teacher_batch.cuda()
         # print("student heatmap batch type "+str(student_heatmap_batch.shape))

          #if experiment == 'NoHeatmap':

            #loss = kl_loss
        #  running_loss += loss.item()
          #running_ap += get_ap_score(torch.Tensor.cpu(labels_batch).detach().numpy(), torch.Tensor.cpu(m(output_batch)).detach().numpy())
          #print("now attempting to declare heatmapbeta")
          heatmapbeta = 1-KLDgamma
          #print("kl_loss shape "+str(kl_loss.shape))
          #print("kl_loss type "+str(kl_loss.dtype))
          #print("kl_loss: "+str(kl_loss))
          #print("heatmap_dissimilarity shape "+str(heatmap_dissimilarity.shape))
          #print("heatmap_dissimilarity type "+str(heatmap_dissimilarity.dtype))
          #print("heatmap_dissimilarity: "+str(heatmap_dissimilarity))
          
          combinedLossTensor = KLDgamma * kl_loss + heatmapbeta * heatmap_dissimilarity
          if KLDgamma == 1:
              combinedLossTensor = kl_loss
              if combinedLossTensor != kl_loss:
                  print("gamma is 1 but the tensors arent'the same:")
                  print("combinedLossTensor:" +str(combinedLossTensor))
                  print("kl_loss: "+str(kl_loss))
          #print("combined loss shape: "+str(combinedLossTensor.shape))
          #print("combined Loss: "+str(combinedLossTensor))
          # clear previous gradients, compute gradients of all variables wrt loss
          #print(type(loss))
          optimizer.zero_grad()
          #loss.backward()
          combinedLossTensor.backward()

          # performs updates using calculated gradients
          optimizer.step()

          # Evaluate summaries only once in a while
          if i % params.save_summary_steps == 0:
              # extract data from torch Variable, move to cpu, convert to numpy arrays
              student_output_batch = student_output_batch.data.cpu().numpy()
              labels_batch = labels_batch.data.cpu().numpy()

              # compute all metrics on this batch
              summary_batch = {metric:metrics[metric](student_output_batch, labels_batch)
                                for metric in metrics}
              #summary_batch['loss'] = loss.data#it was loss.data[0]
              summary_batch['combinedLoss'] = combinedLossTensor.data#it was loss.data[0]
              summary_batch['heatmap_dissimilarity'] = heatmap_dissimilarity
              summary_batch['kl_loss'] = kl_loss.data
              summ.append(summary_batch)
            #  wandb.log({"train_batch_kl_loss":kl_loss.data,"train_batch_heatmap_dissimilarity":heatmap_dissimilarity,
             #            "train_batch_combinedLoss":combinedLossTensor.data,} )

          # update the average loss
          #loss_avg.update(loss.data)#it was loss.data[0]
          combinedLoss_avg.update(combinedLossTensor.data)
         
          #  print("heatmap_dissimilarity "+str(heatmap_dissimilarity))

          heatmap_dissimilarity_avg.update(heatmap_dissimilarity)
        #print("heatmap_dissimilarity average "+str(heatmap_dissimilarity_avg))
          kl_loss_avg.update(kl_loss.data)
         # wandb.log({"train_avg_kl_loss": kl_loss_avg(), "train_avg_heatmap_dissimilarity":heatmap_dissimilarity_avg(),
          #           "train_avg_combinedLoss":combinedLoss_avg() }         )

          t.set_postfix({'train_avg_KL_Loss': '{:05.3f}'.format(float(kl_loss_avg())),
                         'train_avgHeatmap_dissimilarity': '{:05.3f}'.format(float(heatmap_dissimilarity_avg())),
                           'train_avg_combinedLoss': '{:05.3f}'.format(combinedLoss_avg())})
          t.update()
    #      del train_batch, labels_batch, student_output_batch
     #     gc.collect()
      #    torch.cuda.empty_cache()



    # compute mean of all metrics in summary
   # tb.add_scalar('KLD average loss', loss, epoch)
    #tb.add_scalar('Heatmap loss', heatmap_dissimilarity, epoch)
#metrics_mean = {metric:torch.mean(torch.as_tensor([x[metric] for x in summ])) for metric in summ[0]}
    if wandbrun == True:
        wandb.log({"train_avg_KL_Loss": float(kl_loss_avg())})
        wandb.log({"train_avgHeatmap_dissimilarity": float(heatmap_dissimilarity_avg())})
        wandb.log({"train_avg_combinedLoss": combinedLoss_avg()})
    metrics_mean = {
    metric: np.mean(
        [x[metric].cpu().detach().numpy() if isinstance(x[metric], torch.Tensor) else x[metric] for x in summ]
    )
    for metric in summ[0]
    }
    
    
    metrics_string = " ; ".join("{}: {:05.3f}".format(k, v) for k, v in metrics_mean.items())
    logging.info("- Train metrics: " + metrics_string)
    #num_samples = float(len(train_loader.dataset))
    #tr_loss_ = running_loss/num_samples
    #tr_map_ = running_ap/num_samples
    if experiment == 'NoHeatMap':
        print("WARNING: using threshold train_kd method but flag NoHeatmap was given, please use the vanilla version instead")
      #return loss_avg()

  #  print('train_loss: {:.4f}, train_avg_precision:{:.3f}'.format(
    #                 tr_loss_, tr_map_))
    #return loss_avg(), heatmap_dissimilarity_avg() METHOD USED TO RETURN 2 LOSS VALUES BUT MAYBE THIS ISN'T EVEN NECESSARY


                  # Append the values to global arrays
  #   tr_loss.append(tr_loss_), tr_map.append(tr_map_)

def train_kd_nothresh(model, teacher_model, optimizer, loss_fn_kd_nothresh, dataloader, metrics,
                      params, experiment, student_activated_features, teacher_activated_features, KLDgamma, wandbrun):
    """Train the model on `num_steps` batches
    Args:
        model: (torch.nn.Module) the neural network
        optimizer: (torch.optim) optimizer for parameters of model
        loss_fn_kd:
        dataloader:
        metrics: (dict)
        params: (Params) hyperparameters
    """
    #print("inside train_kd_nothresh, experiment: "+str(experiment))
    # set model to training mode
    model.train()
    teacher_model.eval()
    #print("inside train kd no thresh")
    # summary for current training loop and a running average object for loss
    summ = []
    combinedLoss_avg = RunningAverage()
    heatmap_dissimilarity_avg = RunningAverage()
    kl_loss_avg = RunningAverage()
   

    student_model = model

    student_weight_softmax_params =list(student_model.linear.parameters()) # This gives a list of weights for the fully connected layers
    student_weight_softmax = student_weight_softmax_params[0].data
    student_weight_softmax.requires_grad = True
    teacher_weight_softmax_params = list(teacher_model.module.classifier.parameters()) # This gives a list of weights for the fully connected layers
    teacher_weight_softmax = teacher_weight_softmax_params[0].data
    teacher_weight_softmax.requires_grad = True
    # Use tqdm for progress bar
    with tqdm(total=len(dataloader)) as t:
      for i, (train_batch, labels_batch) in enumerate(dataloader):
        #  print("size of train batch "+str(train_batch.shape))
          # move to GPU if available
          if params.cuda:
              train_batch, labels_batch = train_batch.cuda(), \
                                          labels_batch.cuda()
          # convert to torch Variables
          # train_batch, labels_batch = Variable(train_batch), Variable(labels_batch)
          #print("sie of train batch "+str(train_batch.shape))
          # compute model output, fetch teacher output, and compute KD loss
          #output_batch = model(train_batch)

          # get one batch output from teacher_outputs list
          #student_output_batch = model(train_batch)
          if experiment != 'NoHeatmap':

            train_batch, labels_batch = Variable(train_batch, requires_grad= True), Variable(labels_batch)
            teacher_output_batch = teacher_model(train_batch)
             
            
            if params.cuda:
              teacher_output_batch = teacher_output_batch.cuda()

          else:
            train_batch, labels_batch = Variable(train_batch), Variable(labels_batch)
            with torch.no_grad():
              teacher_output_batch = teacher_model(train_batch)
              #print("teacher output batch type after using no grad without going into the cuda part "+str(type(teacher_output_batch)))
              if params.cuda:
                teacher_output_batch = teacher_output_batch.cuda()
               # print("teacher output batch type after using no grad AFTER going into the cuda part "+str(type(teacher_output_batch)))


          student_output_batch = model(train_batch)


          if experiment =='NoHeatmap':
                loss = loss_fn_kd_noHeatMap(outputs=student_output_batch, labels=labels_batch, teacher_outputs=teacher_output_batch, params=params)

          if experiment == 'TrueClass':
            #print("using true classes")
            if (print_if_all_zero(student_activated_features.features)):
              print("student_activated_features.features is all 0s")
             # print("student_activated_features.features: "+str(student_activated_features.features))

            print("student activated features")
            print(student_activated_features.features.grad_fn)
            student_heatmap_batch = getCAMBatch(student_activated_features.features, student_weight_softmax, labels_batch)

            teacher_heatmap_batch = getCAMBatch(teacher_activated_features.features, teacher_weight_softmax, labels_batch)
           # print("one line before kl loss in train kd no thresh")
            kl_loss, heatmap_dissimilarity = loss_fn_kd_nothresh(student_output_batch, labels_batch, teacher_output_batch, params, student_heatmap_batch, teacher_heatmap_batch)

          if experiment == 'TopClass':
           # print("using top class of teacher")
            teacher_preds = torch.sigmoid(teacher_output_batch)
            teacher_pred_probs = F.softmax(teacher_preds, dim=1).data.squeeze()
            max_probs, max_indices = torch.max(teacher_pred_probs, dim=1)
            print("student activated features")
            print(student_activated_features.features.grad_fn)
            student_heatmap_batch = getCAMBatch(student_activated_features.features, student_weight_softmax, max_indices)
            if (check_nan(student_heatmap_batch)):
              print("found a NaN on the student heatmap batch")
            teacher_heatmap_batch = getCAMBatch(teacher_activated_features.features, teacher_weight_softmax, max_indices)
            #print("one line before kl loss in train kd no thresh")
            kl_loss, heatmap_dissimilarity = loss_fn_kd_nothresh(student_output_batch, labels_batch, teacher_output_batch, params, student_heatmap_batch, teacher_heatmap_batch)


          if experiment == 'AllClasses':
            #print("using all classes")
            print("student activated features")
            print(student_activated_features.features.grad_fn)
            student_heatmap_batch = classCAMSbatch(student_activated_features.features, student_weight_softmax)
            teacher_heatmap_batch = classCAMSbatch(teacher_activated_features.features, teacher_weight_softmax)
        #    print("one line before kl loss in train kd no thresh")
            kl_loss, heatmap_dissimilarity = loss_fn_kd_nothresh(student_output_batch, labels_batch, teacher_output_batch, params, student_heatmap_batch, teacher_heatmap_batch)
          #with torch.no_grad():
          #    output_teacher_batch = teacher_model(train_batch)
          #if params.cuda:
           #   output_teacher_batch = output_teacher_batch.cuda()
         # print("student heatmap batch type "+str(student_heatmap_batch.shape))

          #if experiment != 'NoHeatmap':

           # loss = kl_loss
        #  running_loss += loss.item()
          #running_ap += get_ap_score(torch.Tensor.cpu(labels_batch).detach().numpy(), torch.Tensor.cpu(m(output_batch)).detach().numpy())

          heatmapbeta = 1-KLDgamma
          # clear previous gradients, compute gradients of all variables wrt loss
          
          
          combinedLossTensor = KLDgamma * kl_loss + heatmapbeta * (100 * heatmap_dissimilarity)
          
          optimizer.zero_grad()
          combinedLossTensor.backward()
      #    print("type of kl loss" +str(type(kl_loss)))
       #   print("type of heatmap loss "+str(type(heatmap_dissimilarity)))  
        #  print("type of combinedLoss "+str(type(combinedLossTensor)))
         # print("combinedLossTensor "+str(combinedLossTensor))
         # print("kl loss tensor gradient "+str(kl_loss.grad))
          #print("combined loss tensor gradient "+str(combinedLossTensor.grad))
          # performs updates using calculated gradients
          
          # Get gradients before optimizer step
   
    
          
          optimizer.step()
          #gradients_after_step = [param.grad.clone() for param in student_model.parameters()]
          
      #    print("GRADIENTS AFTER STEP: are they all zero?")
       #   for parameter in student_model.parameters():
        #     if torch.all(parameter == 0): 
         #        a =1
          #   else:
           #      print("parameter found (after step) where not everything is 0")
                 #print( "gradient of that parameter")
                 #non_zero_indices = torch.nonzero(parameter)
                 #for index in non_zero_indices:
                 #   value = parameter[index[0]]
                  #  print(f"Non-zero value at index {index}: {value}")
                # print(parameter.grad)   
# Check if gradients are the same
      #    gradients_equal = all(torch.equal(grad_before, grad_after) for grad_before, grad_after in zip(gradients_before_step, gradients_after_step))
       #   if gradients_equal:
        #    print("Gradients are the same before and after optimizer step.")
        #  else:
         #   print("Gradients are different before and after optimizer step.")

          #print("GRADIENTS BEFORE STEP")
          #for parameter in student_model.parameters():
          #    print(parameter.grad)
      
          #print("GRADIENTS AFTER STEP")
          #for parameter in student_model.parameters():
           #   print(parameter.grad)
          # Evaluate summaries only once in a while
          if i % params.save_summary_steps == 0:
              # extract data from torch Variable, move to cpu, convert to numpy arrays
              student_output_batch = student_output_batch.data.cpu().numpy()
              labels_batch = labels_batch.data.cpu().numpy()

              # compute all metrics on this batch
              summary_batch = {metric:metrics[metric](student_output_batch, labels_batch)
                                for metric in metrics}
              summary_batch['combinedLoss'] = combinedLossTensor.data#it was loss.data[0]
              summary_batch['heatmap_dissimilarity'] = heatmap_dissimilarity
              summary_batch['kl_loss'] = kl_loss.data
              summ.append(summary_batch)
              #wandb.log({"train_batch_kl_loss":kl_loss.data,"train_batch_heatmap_dissimilarity":heatmap_dissimilarity,
               #          "train_batch_combinedLoss":combinedLossTensor.data,} )

          # update the average loss
          combinedLoss_avg.update(combinedLossTensor.data)#it was loss.data[0]
          if experiment != 'NoHeatmap':
          #  print("heatmap_dissimilarity "+str(heatmap_dissimilarity))

            heatmap_dissimilarity_avg.update(heatmap_dissimilarity)
            kl_loss_avg.update(kl_loss.data)
            #print("heatmap_dissimilarity average "+str(heatmap_dissimilarity_avg))
        #    wandb.log({"train_avg_kl_loss": kl_loss_avg(), "train_avg_heatmap_dissimilarity":heatmap_dissimilarity_avg(),
         #            "train_avg_combinedLoss":combinedLoss_avg() }         )
            t.set_postfix({'train_average_KL_Loss': '{:05.3f}'.format(float(kl_loss_avg())),
                           'train_avgHeatmap_dissimilarity': '{:05.3f}'.format(float(heatmap_dissimilarity_avg())),
                        'train_avg_combinedLoss': '{:05.3f}'.format(combinedLoss_avg())})
          else :
            print("you're using a non vanilla train_kd method, you should call the vanilla method instead since you're not using heatmaps")
            t.set_postfix(loss='{:05.3f}'.format(combinedLoss_avg()))
          t.update()
  
        
    #      del train_batch, labels_batch, student_output_batch
     #     gc.collect()
      #    torch.cuda.empty_cache()



    # compute mean of all metrics in summary
   # tb.add_scalar('KLD average loss', loss, epoch)
    #tb.add_scalar('Heatmap loss', heatmap_dissimilarity, epoch)
    #metrics_mean = {metric:torch.mean(torch.as_tensor([x[metric] for x in summ])) for metric in summ[0]}
    if wandbrun == True:
        wandb.log({"train_avg_KL_Loss": float(kl_loss_avg())})
        wandb.log({"train_avgHeatmap_dissimilarity": float(heatmap_dissimilarity_avg())})
        wandb.log({"train_avg_combinedLoss": combinedLoss_avg()})
        wandb.log({"first layer weight grad": student_model[0].weights.grad})
        wandb.log({"first layer bias grad": student_model[0].bias.grad}) 
        wandb.log({"last layer weight grad": student_model[-1].weights.grad})
        wandb.log({"last layer bias grad": student_model[-1].bias.grad})
    metrics_mean = {
    metric: np.mean(
        [x[metric].cpu().detach().numpy() if isinstance(x[metric], torch.Tensor) else x[metric] for x in summ]
    )
    for metric in summ[0]
    }
    
    
    metrics_string = " ; ".join("{}: {:05.3f}".format(k, v) for k, v in metrics_mean.items())
    logging.info("- Train metrics: " + metrics_string)
  
  
  
  #  writer = SummaryWriter("torchlogs/")
   # writer.add_graph(student_model, train_batch)
    #writer.close()
    #num_samples = float(len(train_loader.dataset))
    #tr_loss_ = running_loss/num_samples
    #tr_map_ = running_ap/num_samples
    #if experiment == 'NoHeatMap':
    #  return loss_avg()

  #  print('train_loss: {:.4f}, train_avg_precision:{:.3f}'.format(
    #                 tr_loss_, tr_map_))
    #return loss_avg(), heatmap_dissimilarity_avg()


                  # Append the values to global arrays
  #   tr_loss.append(tr_loss_), tr_map.append(tr_map_)


def evaluate_kd_thresh(model, teacher_model, dataloader, metrics, params, experiment, student_activated_features, teacher_activated_features,threshold, KLDgamma):
    """Evaluate the model on `num_steps` batches.
    Args:
        model: (torch.nn.Module) the neural network
        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch
        dataloader: (DataLoader) a torch.utils.data.DataLoader object that fetches data
        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch
        params: (Params) hyperparameters
        num_steps: (int) number of batches to train on, each of size params.batch_size
    """

    # set model to evaluation mode
    model.eval()

    # summary for current eval loop
    summ = []
    student_model = model
    student_weight_softmax_params =list(student_model.linear.parameters()) # This gives a list of weights for the fully connected layers
    student_weight_softmax = student_weight_softmax_params[0].data
    student_weight_softmax.requires_grad = True
    teacher_weight_softmax_params = list(teacher_model.module.classifier.parameters()) # This gives a list of weights for the fully connected layers
    teacher_weight_softmax = teacher_weight_softmax_params[0].data
    teacher_weight_softmax.requires_grad = True

    m = torch.nn.Sigmoid()


    # compute metrics over the dataset
    with torch.no_grad():
      for i, (data_batch, labels_batch) in enumerate(dataloader):

          # move to GPU if available
          #print("shape of label batch"+str(labels_batch.shape))
        # print("shape of data_batch"+str(data_batch.shape))
          if params.cuda:
              data_batch, labels_batch = data_batch.cuda(), labels_batch.cuda()
          # fetch the next evaluation batch
          data_batch, labels_batch = Variable(data_batch), Variable(labels_batch)
          # compute model output

          student_output_batch = model(data_batch)
          teacher_output_batch = teacher_model(data_batch)
         # loss = loss_fn_kd(outputs= output_batch, labels = labels_batch, teacher_outputs =output_teacher_batch, params=params)
          #loss = 0.0  #force validation loss to zero to reduce computation time
          if experiment == 'TrueClass':
            student_heatmap_batch = getCAMBatch(student_activated_features.features, student_weight_softmax, labels_batch)

            teacher_heatmap_batch = getCAMBatch(teacher_activated_features.features, teacher_weight_softmax, labels_batch)


          if experiment == 'TopClass':
            teacher_preds = torch.sigmoid(teacher_output_batch)
            teacher_pred_probs = F.softmax(teacher_preds, dim=1).data.squeeze()
            max_probs, max_indices = torch.max(teacher_pred_probs, dim=1)
            student_heatmap_batch = getCAMBatch(student_activated_features.features, student_weight_softmax, max_indices)
            teacher_heatmap_batch = getCAMBatch(teacher_activated_features.features, teacher_weight_softmax, max_indices)


          if experiment == 'AllClasses':
            student_heatmap_batch = classCAMSbatch(student_activated_features.features, student_weight_softmax)
            teacher_heatmap_batch = classCAMSbatch(teacher_activated_features.features, teacher_weight_softmax)


          with torch.no_grad():
            kl_loss, heatmap_dissimilarity = loss_fn_kd_thresh(student_output_batch, labels_batch, teacher_output_batch, params, student_heatmap_batch, teacher_heatmap_batch, threshold)   
            heatmapbeta = 1-KLDgamma
            combinedLossTensor = KLDgamma * kl_loss + heatmapbeta * heatmap_dissimilarity
          output_batch = student_output_batch
          # extract data from torch Variable, move to cpu, convert to numpy arrays
          output_batch = output_batch.data.cpu().numpy()
          labels_batch = labels_batch.data.cpu().numpy()

          # compute all metrics on this batch
          summary_batch = {metric: metrics[metric](output_batch, labels_batch)
                          for metric in metrics}
          # summary_batch['loss'] = loss.data[0]
          summary_batch['combinedLoss'] = combinedLossTensor.data
          if experiment !='NoHeatmap':
            summary_batch['heatmap_dissimilarity'] = heatmap_dissimilarity
            summary_batch['kl_loss'] = kl_loss.data
          summ.append(summary_batch)
         # wandb.log({"val_batch_kl_loss": kl_loss.data, "val_batch_heatmap_dissimilarity": heatmap_dissimilarity,
          #           "val_batch_combinedLoss":combinedLossTensor.data})  

          #del data_batch, labels_batch, output_batch
          #gc.collect()x
          #torch.cuda.empty_cache()


    # compute mean of all metrics in summary
    #metrics_mean = {metric:torch.mean(torch.as_tensor([x[metric] for x in summ])) for metric in summ[0]} #changef from np.mean to torch.mean
    metrics_mean = {
    metric: np.mean(
        [x[metric].cpu().detach().numpy() if isinstance(x[metric], torch.Tensor) else x[metric] for x in summ]
    )
    for metric in summ[0]
    }
    
    
    metrics_string = " ; ".join("{}: {:05.3f}".format(k, v) for k, v in metrics_mean.items())
    logging.info("- Eval metrics : " + metrics_string)

    return metrics_mean


def evaluate_kd_nothresh(model, teacher_model, dataloader, metrics, params, experiment, student_activated_features, teacher_activated_features, KLDgamma):
    """Evaluate the model on `num_steps` batches.
    Args:
        model: (torch.nn.Module) the neural network
        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch
        dataloader: (DataLoader) a torch.utils.data.DataLoader object that fetches data
        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch
        params: (Params) hyperparameters
        num_steps: (int) number of batches to train on, each of size params.batch_size
    """

    # set model to evaluation mode
    model.eval()

    # summary for current eval loop
    summ = []
    
    student_weight_softmax_params =list(model.linear.parameters()) # This gives a list of weights for the fully connected layers
    student_weight_softmax = student_weight_softmax_params[0].data
    student_weight_softmax.requires_grad = True
    teacher_weight_softmax_params = list(teacher_model.module.classifier.parameters()) # This gives a list of weights for the fully connected layers
    teacher_weight_softmax = teacher_weight_softmax_params[0].data
    teacher_weight_softmax.requires_grad = True
    
    m = torch.nn.Sigmoid()


    # compute metrics over the dataset
    with torch.no_grad():
      for i, (data_batch, labels_batch) in enumerate(dataloader):

          # move to GPU if available
          #print("shape of label batch"+str(labels_batch.shape))
        # print("shape of data_batch"+str(data_batch.shape))
          if params.cuda:
              data_batch, labels_batch = data_batch.cuda(), labels_batch.cuda()
          # fetch the next evaluation batch
          data_batch, labels_batch = Variable(data_batch), Variable(labels_batch)
          # compute model output

          student_output_batch = model(data_batch)
          teacher_output_batch = teacher_model(data_batch)
         # loss = loss_fn_kd(outputs= output_batch, labels = labels_batch, teacher_outputs =output_teacher_batch, params=params)
          #loss = 0.0  #force validation loss to zero to reduce computation time
          if experiment == 'TrueClass':
            student_heatmap_batch = getCAMBatch(student_activated_features.features, student_weight_softmax, labels_batch)

            teacher_heatmap_batch = getCAMBatch(teacher_activated_features.features, teacher_weight_softmax, labels_batch)


          if experiment == 'TopClass':
            teacher_preds = torch.sigmoid(teacher_output_batch)
            teacher_pred_probs = F.softmax(teacher_preds, dim=1).data.squeeze()
            max_probs, max_indices = torch.max(teacher_pred_probs, dim=1)
            student_heatmap_batch = getCAMBatch(student_activated_features.features, student_weight_softmax, max_indices)
            teacher_heatmap_batch = getCAMBatch(teacher_activated_features.features, teacher_weight_softmax, max_indices)
           # print("made teacher and student heatmap batches")

          if experiment == 'AllClasses':
            student_heatmap_batch = classCAMSbatch(student_activated_features.features, student_weight_softmax)
            teacher_heatmap_batch = classCAMSbatch(teacher_activated_features.features, teacher_weight_softmax)


          with torch.no_grad():
            #print("attempting loss")  
            kl_loss, heatmap_dissimilarity = loss_fn_kd_nothresh(student_output_batch, labels_batch, teacher_output_batch, params, student_heatmap_batch, teacher_heatmap_batch)
            #print("loss made")
            heatmapbeta = 1-KLDgamma
            combinedLossTensor = KLDgamma * kl_loss + heatmapbeta * heatmap_dissimilarity
          

         # running_loss += loss.item() # sum up batch loss
          #running_ap += get_ap_score(torch.Tensor.cpu(labels_batch).detach().numpy(), torch.Tensor.cpu(m(output_batch)).detach().numpy())

          output_batch = student_output_batch
          # extract data from torch Variable, move to cpu, convert to numpy arrays
          output_batch = output_batch.data.cpu().numpy()
          labels_batch = labels_batch.data.cpu().numpy()

          # compute all metrics on this batch
          summary_batch = {metric: metrics[metric](output_batch, labels_batch)
                          for metric in metrics}
          # summary_batch['loss'] = loss.data[0]
          summary_batch['combinedLoss'] = combinedLossTensor.data
          if experiment !='NoHeatmap':
            summary_batch['heatmap_dissimilarity'] = heatmap_dissimilarity
            summary_batch['kl_loss'] = kl_loss.data
          summ.append(summary_batch)
        #  wandb.log({"val_batch_kl_loss": kl_loss.data, "val_batch_heatmap_dissimilarity": heatmap_dissimilarity,
         #           "val_batch_combinedLoss":combinedLossTensor.data})  



          #del data_batch, labels_batch, output_batch
          #gc.collect()
          #torch.cuda.empty_cache()


    # compute mean of all metrics in summary
    #metrics_mean = {metric:torch.mean(torch.as_tensor([x[metric] for x in summ])) for metric in summ[0]} #changef from np.mean to torch.mean
    metrics_mean = {
    metric: np.mean(
        [x[metric].cpu().detach().numpy() if isinstance(x[metric], torch.Tensor) else x[metric] for x in summ]
    )
    for metric in summ[0]
    }
    
    
    metrics_string = " ; ".join("{}: {:05.3f}".format(k, v) for k, v in metrics_mean.items())
    logging.info("- Eval metrics : " + metrics_string)

    return metrics_mean

def train_and_evaluate_kd_thresh(model, teacher_model, train_dataloader, val_dataloader, optimizer,
                      # loss_fn_kd_thresh, metrics, params, model_dir, restore_file, runconfig):
                loss_fn_kd_thresh, metrics, params, threshold, student_arch, teacher_arch, restore_file=None,experiment='TrueClass',
                KLDgamma = 1, wandbrun= False):
                       #loss_fn_kd_thresh, metrics, params, model_dir, threshold, restore_file=None,experiment='TrueClass'):
                         
    """Train the model and evaluate every epoch.
    Args:
        model: (torch.nn.Module) the neural network
        params: (Params) hyperparameters
        model_dir: (string) directory containing config, weights and log
        restore_file: (string) - file to restore (without its extension .pth.tar)    
    """
  #  with wandb.init(config=runconfig):
   #     config = wandb.config
    # If called by wandb.agent, as below,
    # this config will be set by Sweep Controller
     
    # reload weights from restore_file if specified
    if restore_file is not None:
        restore_path = os.path.join(model_dir, restore_file + '.pth')
        logging.info("Restoring parameters from {}".format(restore_path))
        load_checkpoint(restore_path, model, optimizer)



    student_model = model
    best_val_acc = 0.0
    student_final_layer = model.layer4[-1]
    teacher_final_layer = teacher_model.module.stage_3.stage_3_bottleneck_2 # Grab the final layer of the model
    #print("config: "+str(config))
    if experiment !='NoHeatmap':

        student_activated_features = SaveFeatures(student_final_layer)
        student_activated_features.requires_grad = True
        teacher_activated_features = SaveFeatures(teacher_final_layer) # attach the call back hook to the final layer of the model
        teacher_activated_features.requires_grad = True
    #print("student final layer: "+str(student_final_layer))
    # Tensorboard logger setup
    board_logger = Board_Logger(os.path.join(model_dir, 'board_logs'))

    # learning rate schedulers for different models:
    if params.model_version == "resnet18_distill":
        scheduler = StepLR(optimizer, step_size=30, gamma=0.1)
    # for cnn models, num_epoch is always < 100, so it's intentionally not using scheduler here
    elif params.model_version == "cnn_distill":
        scheduler = StepLR(optimizer, step_size=30, gamma=0.2)
    if KLDgamma == 1:
        print("KLD gamma = 1:  heatmap dissimilarity will be tracked but will have no impact on the loss calculation")
        
    if KLDgamma == 0:
         print("KLD gamma = 0: KL_loss will be tracked but will have no impact on the loss calculation, only heatmap dissimilarity will matter")
            
    print("KLDGamma: "+str(KLDgamma))
    print("threshold: "+str(threshold))  
    print("experiment: "+str(experiment))  
    for epoch in range(params.num_epochs):
    #  print("memory summary before starting new epoch:")
    # print(torch.cuda.memory_summary())    
        print("-------Epoch {}----------".format(epoch+1))
        scheduler.step()
       # print("epoch has begun I guess")

        # Run one epoch
        logging.info("Epoch {}/{}".format(epoch + 1, params.num_epochs))

        # compute number of batches in one epoch (one full pass over the training set)
        if experiment != 'NoHeatmap':
            #epochnum = epoch +1 
            train_kd_thresh(model, teacher_model, optimizer, loss_fn_kd_thresh, train_dataloader,
                metrics, params, experiment, student_activated_features, teacher_activated_features, threshold, KLDgamma, wandbrun)
            #  wandb.log({"epoch":epoch})    
        # Evaluate for one epoch on validation set
        print("KLDgamma: "+str(KLDgamma))
        val_metrics = evaluate_kd_thresh(student_model, teacher_model, val_dataloader, metrics, params, experiment, student_activated_features, 
                                            teacher_activated_features,threshold, KLDgamma)
        
        val_acc = val_metrics['accuracy']
        print("validation accuracy: "+str(val_acc))
        val_combinedLoss = val_metrics['combinedLoss']
        print("validation combinedLoss: "+str(val_combinedLoss))
        val_kl_loss = val_metrics['kl_loss']
        print("validation kl_loss: "+str(val_kl_loss))
        val_heatmap_dissimilarity = val_metrics['heatmap_dissimilarity']
        print("validation heatmap_dissimilarity: "+str(val_heatmap_dissimilarity))
        if wandbrun == True:

           wandb.log({"val_acc": val_acc, "epoch": epoch+1})       
           wandb.log({"val_combined_loss": val_combinedLoss, "epoch": epoch+1})       
           wandb.log({"val_kl_loss": val_kl_loss, "epoch": epoch+1})       
           wandb.log({"val_heatmap_dissimilarity": val_heatmap_dissimilarity, "epoch": epoch+1})      
        # Save weights
        #wandb.log({"val_acc": val_acc, "epoch": epoch})       
        #wandb.log({"val_combined_loss": val_combinedLoss, "epoch": epoch})       
        #wandb.log({"val_kl_loss": val_kl_loss, "epoch": epoch})       
        #wandb.log({"val_heatmap_dissimilarity": val_heatmap_dissimilarity, "epoch": epoch})       
        #print()
        is_best = val_acc>=best_val_acc
        #print("validation accuracy: "+str(val_acc))
        # Save weights
        
        
        settings_dict ={
            "useKD": True,
            "student_model": student_arch,
            "teacher_model": teacher_arch,
            "usethreshold": True,
            "threshold": threshold,
            "experiment": experiment,
            "KLDgamma": KLDgamma            
        }
        gammastring = int(KLDgamma * 100)
        threshstring = int(threshold *100)
        path_base = "/bin/smartinez/kdfiles/KD/thresh"
        destination_folder = f'{path_base}/student_{student_arch}/teacher_{teacher_arch}/experiment_{experiment}/KLDgamma_{gammastring}/threshold_{threshstring}' 
        path_base = "/home/smartinez/modelruns"
       # destination_folder = f'{path_base}/student_{student_arch}/teacher_{teacher_arch}/experiment_{experiment}/KLDgamma_{gammastring}/' 
        destination_folder = f'{path_base}/KD_thresh_student_{student_arch}_teacher_{teacher_arch}_experiment_{experiment}_KLDgamma_{gammastring}_threshold_{threshstring}' 
    
    
        # If best_eval, best_save_path
        if is_best:
            logging.info("- Found new best accuracy     ")
            best_val_acc = val_acc
            print("best_val_acc: "+str(best_val_acc))
            save_checkpoint({'epoch': epoch + 1,
                            'state_dict': model.state_dict(),
                            'optim_dict' : optimizer.state_dict()},
                            is_best=is_best,
                            checkpoint=destination_folder)
            print("saving new best weights to "+ destination_folder)
            if wandbrun == True:
                
                wandb.log({"best_val_acc": best_val_acc, "epoch": epoch+1})
            #wandb.log({"best_val_acc": best_val_acc, "epoch": epoch})conda
            # Save best val metrics in a json file in the model directory
            best_json_path = os.path.join(model_dir, "metrics_val_best_weights.json")
            save_dict_to_json(val_metrics, best_json_path)

        # Save latest val metrics in a json file in the model directory
        last_json_path = os.path.join(model_dir, "metrics_val_last_weights.json")
        save_dict_to_json(val_metrics, last_json_path)



        # #============ TensorBoard logging: uncomment below to turn in on ============#
        #DO NOT ACTIVATE THIS IF YOU WANT PEACE, not even 40GB VRAM are enough
        # # (1) Log the scalar values
        #info = {
        #   'val accuracy': val_acc,
            #'val KL  loss': val_kl_loss,
            #'val Heatmap loss': val_heatmap_dissimilarity,
            # 'val combined Loss': val_combinedLoss

        #}

    #  for tag, value in info.items():
    #     board_logger.scalar_summary(tag, value, epoch+1)

        # # (2) Log values and gradients of the parameters (histogram)
        #for tag, value in model.named_parameters():
        #   tag = tag.replace('.', '/')
        #  board_logger.histo_summary(tag, value.data.cpu().numpy(), epoch+1)
        # board_logger.histo_summary(tag+'/grad', value.grad.data.cpu().numpy(), epoch+1)



    teacher_activated_features.remove()
    student_activated_features.remove()


     
                     

def train_and_evaluate_kd_nothresh(model, teacher_model, train_dataloader, val_dataloader, optimizer,    #  loss_fn_kd_nothresh, metrics, params, model_dir, restore_file, runconfig)
                       loss_fn_kd_nothresh, metrics, params, student_arch, teacher_arch, restore_file=None,experiment='TrueClass', KLDgamma=1, wandbrun= False):
    """Train the model and evaluate every epoch.
    Args:
        model: (torch.nn.Module) the neural network
        params: (Params) hyperparameters
        model_dir: (string) directory containing config, weights and log
        restore_file: (string) - file to restore (without its extension .pth.tar)
    """
    #with wandb.init(config=runconfig):
     #   config = wandb.config
    # reload weights from restore_file if specified
   
    student_model = model
    images, labels = next(iter(train_dataloader))
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    images, labels = images.to(device), labels.to(device)
#grid = torchvision.utils.make_grid(images)

#tb.add_image('images', grid)
    tb= SummaryWriter(log_dir='/home/smartinez/torchlogs')
    tb.add_graph(student_model, images)
    tb.close()
    if restore_file is not None:
        restore_path = os.path.join(model_dir, restore_file + '.pth')
        logging.info("Restoring parameters from {}".format(restore_path))
        load_checkpoint(restore_path, model, optimizer)




    best_val_acc = 0.0
    
    student_final_layer = student_model.layer4[-1]
    teacher_final_layer = teacher_model.module.stage_3.stage_3_bottleneck_2 # Grab the final layer of the model

    if experiment !='NoHeatmap':

        student_activated_features = SaveFeatures(student_final_layer)
        #student_activated_features.requires_grad = True
        
        teacher_activated_features = SaveFeatures(teacher_final_layer) # attach the call back hook to the final layer of the model
       # teacher_activated_features.requires_grad = True
    #print("student final layer: "+str(student_final_layer))
    # Tensorboard logger setup
    board_logger = Board_Logger(os.path.join(model_dir, 'board_logs'))

    # learning rate schedulers for different models:
    if params.model_version == "resnet18_distill":
        scheduler = StepLR(optimizer, step_size=30, gamma=0.1)
    # for cnn models, num_epoch is always < 100, so it's intentionally not using scheduler here
    elif params.model_version == "cnn_distill":
        scheduler = StepLR(optimizer, step_size=30, gamma=0.2)
    if KLDgamma == 1: 
        print("KLD gamma = 1:  heatmap dissimilarity will be tracked but will have no impact on the loss calculation")
        
    if KLDgamma == 0:
         print("KLD gamma = 0: KL_loss will be tracked but will have no impact on the loss calculation, only heatmap dissimilarity will matter")
           
    for epoch in range(params.num_epochs):
    #  print("memory summary before starting new epoch:")
    # print(torch.cuda.memory_summary())    
        print("-------Epoch {}----------".format(epoch+1))
        scheduler.step()

        # Run one epoch
        logging.info("Epoch {}/{}".format(epoch + 1, params.num_epochs))

        # compute number of batches in one epoch (one full pass over the training set)
        if experiment != 'NoHeatmap':
            train_kd_nothresh(model, teacher_model, optimizer, loss_fn_kd_nothresh, train_dataloader,
                metrics, params, experiment, student_activated_features, teacher_activated_features, KLDgamma, wandbrun)
            #  wandb.log({"epoch":epoch})

        else:
            train_kd_vanilla(model,teacher_model,optimizer,loss_fn_kd_noHeatMap,train_dataloader,metrics,params)
        # Evaluate for one epoch on validation set
        #print("memory summary after a round of training but before evaluating")
        #print(torch.cuda.memory_summary())
        
        val_metrics = evaluate_kd_nothresh(model, teacher_model, val_dataloader, metrics, params, experiment, 
                                            student_activated_features, teacher_activated_features, KLDgamma)
        #print("memory summary after evaluating")
        #print(torch.cuda.memory_summary())
        
        val_acc = val_metrics['accuracy']
        print("validation accuracy: "+str(val_acc))
        val_combinedLoss = val_metrics['combinedLoss']
        print("validation combinedLoss: "+str(val_combinedLoss))
        val_kl_loss = val_metrics['kl_loss']
        print("validation kl_loss: "+str(val_kl_loss))
        val_heatmap_dissimilarity = val_metrics['heatmap_dissimilarity']
        print("validation heatmap_dissimilarity: "+str(val_heatmap_dissimilarity))
        if wandbrun == True:

           wandb.log({"val_acc": val_acc, "epoch": epoch+1})       
           wandb.log({"val_combined_loss": val_combinedLoss, "epoch": epoch+1})       
           wandb.log({"val_kl_loss": val_kl_loss, "epoch": epoch+1})       
           wandb.log({"val_heatmap_dissimilarity": val_heatmap_dissimilarity, "epoch": epoch+1})      
        # Save weights
        
        is_best = val_acc>=best_val_acc
        
        
        
        settings_dict ={
            "useKD": True,
            "student_model": student_arch,
            "teacher_model": teacher_arch,
            "usethreshold": False,
            "experiment": experiment,
            "KLDgamma": KLDgamma           
        }
        
        gammastring = int(KLDgamma * 100)
        path_base = "/home/smartinez/modelruns"
       # destination_folder = f'{path_base}/student_{student_arch}/teacher_{teacher_arch}/experiment_{experiment}/KLDgamma_{gammastring}/' 
        destination_folder = f'{path_base}/kd_nothresh_student_{student_arch}_teacher_{teacher_arch}_experiment_{experiment}_KLDgamma_{gammastring}' 
    
        # If best_eval, best_save_path
        if is_best:
            logging.info("- Found new best accuracy     ")
            print("previous best validation accuracy: "+str(best_val_acc))
            best_val_acc = val_acc
            print("new best validation accuracy: "+str(best_val_acc))
            save_checkpoint({'epoch': epoch + 1,
                            'state_dict': model.state_dict(),
                            'optim_dict' : optimizer.state_dict()},
                            is_best=is_best,
                            checkpoint=destination_folder)
            print("saved new best weights to "+ destination_folder)
            if wandbrun == True:
                
                wandb.log({"best_val_acc": best_val_acc, "epoch": epoch+1})
            # Save best val metrics in a json file in the model directory
            best_json_path = os.path.join(model_dir, "metrics_val_best_weights.json")
            save_dict_to_json(val_metrics, best_json_path)

        # Save latest val metrics in a json file in the model directory
        last_json_path = os.path.join(model_dir, "metrics_val_last_weights.json")
        save_dict_to_json(val_metrics, last_json_path)


        #UNLESS YOU HAVE AT LEAST 300GB VRAM OR MORE NEVER ACTIVATE THIS
        # #============ TensorBoard logging: uncomment below to turn in on ============#
        # # (1) Log the scalar values
    # info = {
                #            'val accuracy': val_acc,
            # 'KL avg loss': loss,
            #'Heatmap loss': heatmap_dissimilarity
        #     'val accuracy': val_acc,
        #    'val KL  loss': val_kl_loss,
        #   'val Heatmap loss': val_heatmap_dissimilarity,
        #  'val combined Loss': val_combinedLoss

        #}

        # }

        #for tag, value in info.items():
        #    board_logger.scalar_summary(tag, value, epoch+1)

        # # (2) Log values and gradients of the parameters (histogram)
        #for tag, value in model.named_parameters():
        # tag = tag.replace('.', '/')
        #  board_logger.histo_summary(tag, value.data.cpu().numpy(), epoch+1)
        # board_logger.histo_summary(tag+'/grad', value.grad.data.cpu().numpy(), epoch+1)



    teacher_activated_features.remove()
    student_activated_features.remove()


def modelrun(model, teacher_model, train_dataloader, val_dataloader, optimizer,
                loss_fn, metrics, params, threshold,student_arch, teacher_arch, usethresh= False, useKD = True, restore_file=None,experiment='TrueClass',
                KLDgamma = 1, wandbrun= False):
    if useKD == False:
        print("doing vanilla run without KD")   
        train_and_evaluate(model, train_dataloader, val_dataloader, optimizer,
                       loss_fn, metrics, params, student_arch, restore_file, wandbrun)
        
    if usethresh == True:
        print("doing run with threshold KD")
        train_and_evaluate_kd_thresh(model, teacher_model, train_dataloader, val_dataloader, optimizer,
                loss_fn_kd_thresh, metrics, params, threshold, student_arch, teacher_arch, restore_file,experiment,KLDgamma, wandbrun)
    elif usethresh == False:
        print("doing KD run without threshold")
        train_and_evaluate_kd_nothresh(model, teacher_model, train_dataloader, val_dataloader, optimizer,
                       loss_fn_kd_nothresh, metrics, params, student_arch, teacher_arch,  restore_file,experiment, KLDgamma, wandbrun)
        
        
class RunningAverage():
    """A simple class that maintains the running average of a quantity

    Example:
    ```
    loss_avg = RunningAverage()
    loss_avg.update(2)
    loss_avg.update(4)
    loss_avg() = 3
    ```
    """
    def __init__(self):
        self.steps = 0
        self.total = 0

    def update(self, val):
        self.total += val
        self.steps += 1

    def __call__(self):
        return self.total/float(self.steps)


def set_logger(log_path):
    """Set the logger to log info in terminal and file `log_path`.
    In general, it is useful to have a logger so that every output to the terminal is saved
    in a permanent file. Here we save it to `model_dir/train.log`.
    Example:
    ```
    logging.info("Starting training...")
    ```
    Args:
        log_path: (string) where to log
    """
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)

    if not logger.handlers:
        # Logging to a file
        file_handler = logging.FileHandler(log_path)
        file_handler.setFormatter(logging.Formatter('%(asctime)s:%(levelname)s: %(message)s'))
        logger.addHandler(file_handler)

        # Logging to console
        stream_handler = logging.StreamHandler()
        stream_handler.setFormatter(logging.Formatter('%(message)s'))
        logger.addHandler(stream_handler)


def save_dict_to_json(d, json_path):
    """Saves dict of floats in json file
    Args:
        d: (dict) of float-castable values (np.float, int, float, etc.)
        json_path: (string) path to json file
    """
    with open(json_path, 'w') as f:
        # We need to convert the values to float for json (it doesn't accept np.array, np.float, )
        d = {k: float(v) for k, v in d.items()}
        json.dump(d, f, indent=4)

def save_weights(filename, model):
    state = model.state_dict()
    torch.save(state, filename)

def save_checkpoint(state, is_best, checkpoint):
    """Saves model and training parameters at checkpoint + 'last.pth.tar'. If is_best==True, also saves
    checkpoint + 'best.pth.tar'
    Args:
        state: (dict) contains model's state_dict, may contain other keys such as epoch, optimizer state_dict
        is_best: (bool) True if it is the best model seen till now
        checkpoint: (string) folder where parameters are to be saved
    """
    
   # filepath = os.path.join(checkpoint, '_last.pth.tar')
    filepath = f'{checkpoint}_last.pth.tar'
    print("filepath: "+str(filepath))
    if not os.path.exists(checkpoint):
        print("Checkpoint Directory does not exist! {}".format(checkpoint))
        #os.mkdir(checkpoint)
       # print("directory created: "+str(checkpoint))
        
    else:
        print("Checkpoint Directory exists! ")

    print("attempting to save to "+str(filepath))
    torch.save(state, filepath)
    if is_best:
       # shutil.copyfile(filepath, os.path.join(checkpoint, '_best.pth.tar'))
       try:
        shutil.copyfile(filepath,  f'{checkpoint}_best.pth.tar')
       except shutil.SameFileError:
        print("same file error")   
        pass
      # shutil.copyfile(filepath,  f'{checkpoint}_last.pth.tar')



def load_checkpoint(checkpoint, model, optimizer=None):
    """Loads model parameters (state_dict) from file_path. If optimizer is provided, loads state_dict of
    optimizer assuming it is present in checkpoint.
    Args:
        checkpoint: (string) filename which needs to be loaded
        model: (torch.nn.Module) model for which the parameters are loaded
        optimizer: (torch.optim) optional: resume optimizer from checkpoint
    """
    if not os.path.exists(checkpoint):
        raise("File doesn't exist {}".format(checkpoint))
    if torch.cuda.is_available():
        checkpoint = torch.load(checkpoint)
    else:
        # this helps avoid errors when loading single-GPU-trained weights onto CPU-model
        checkpoint = torch.load(checkpoint, map_location=lambda storage, loc: storage)

    model.load_state_dict(checkpoint['state_dict'])

    if optimizer:
        optimizer.load_state_dict(checkpoint['optim_dict'])

    return checkpoint


class Board_Logger(object):
    """Tensorboard log utility"""

    def __init__(self, log_dir):
        """Create a summary writer logging to log_dir."""
        self.writer = tf.summary.create_file_writer(log_dir)

    def scalar_summary(self, tag, value, step):
        """Log a scalar variable."""
        #summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])
        #self.writer.add_summary(summary, step)
        with self.writer.as_default():
            tf.summary.scalar(tag, value, step=step)

    def image_summary(self, tag, images, step):
        """Log a list of images."""

        #img_summaries = []
        #for i, img in enumerate(images):
            # Write the image to a string
        #    try:
         #       s = StringIO()
          #  except:
          #      s = BytesIO()
          #  scipy.misc.toimage(img).save(s, format="png")

            # Create an Image object
           # img_sum = tf.Summary.Image(encoded_image_string=s.getvalue(),
            #                           height=img.shape[0],
             #                          width=img.shape[1])
            # Create a Summary value
            #img_summaries.append(tf.Summary.Value(tag='%s/%d' % (tag, i), image=img_sum))

        # Create and write Summary
        #summary = tf.Summary(value=img_summaries)
        #self.writer.add_summary(summary, step)
        with self.writer.as_default():
          img_summaries = []
          for i, img in enumerate(images):
              # Create an Image object
              img_sum = tf.summary.image(tag='%s/%d' % (tag, i), data=img, step=step)
              img_summaries.append(img_sum)

          tf.summary.experimental.write_images(img_summaries, step=step, max_outputs=len(images))


    def histo_summary(self, tag, values, step, bins=1000):
        """Log a histogram of the tensor of values."""
        with self.writer.as_default():
            tf.summary.histogram(tag, values, step=step, buckets=bins)

        self.writer.flush()

        # Create a histogram using numpy
      #  counts, bin_edges = np.histogram(values, bins=bins)

        # Fill the fields of the histogram proto
       # hist = tf.HistogramProto()
        #hist.min = float(np.min(values))
        #hist.max = float(np.max(values))
        #hist.num = int(np.prod(values.shape))
        #hist.sum = float(np.sum(values))
        #hist.sum_squares = float(np.sum(values**2))

        # Drop the start of the first bin


"""
   CIFAR-10 data normalization reference:
   https://github.com/Armour/pytorch-nn-practice/blob/master/utils/meanstd.py
"""

import random
import os
import numpy as np
from PIL import Image
import torch
import torchvision
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler

def fetch_dataloader(types, params):
    """
    Fetch and return train/dev dataloader with hyperparameters (params.subset_percent = 1.)
    """

    # using random crops and horizontal flip for train set
    if params.augmentation == "yes":
        train_transformer = transforms.Compose([
            transforms.RandomCrop(32, padding=4),
            transforms.RandomHorizontalFlip(),  # randomly flip image horizontally
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])

    # data augmentation can be turned off
    else:
        train_transformer = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])

    # transformer for dev set
    dev_transformer = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])

    trainset = torchvision.datasets.CIFAR10(root='/home/shared/data/cifar10', train=True,
        download=True, transform=train_transformer)
    trainloader = torch.utils.data.DataLoader(trainset, batch_size=params.batch_size,
        shuffle=True, num_workers=params.num_workers, pin_memory=params.cuda)

    devset = torchvision.datasets.CIFAR10(root='/home/shared/data/cifar10', train=False,
        download=True, transform=dev_transformer)
    devloader = torch.utils.data.DataLoader(devset, batch_size=params.batch_size,
        shuffle=False, num_workers=params.num_workers, pin_memory=params.cuda)

    if types == 'train':
        dl = trainloader
    else:
        dl = devloader

    return dl


def fetch_subset_dataloader(types, params):
    """
    Use only a subset of dataset for KD training, depending on params.subset_percent
    """

    # using random crops and horizontal flip for train set
    if params.augmentation == "yes":
        train_transformer = transforms.Compose([
            transforms.RandomCrop(32, padding=4),
            transforms.RandomHorizontalFlip(),  # randomly flip image horizontally
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])

    # data augmentation can be turned off
    else:
        train_transformer = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])

    # transformer for dev set
    dev_transformer = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])

    trainset = torchvision.datasets.CIFAR10(root='./data-cifar10', train=True,
        download=True, transform=train_transformer)

    devset = torchvision.datasets.CIFAR10(root='./data-cifar10', train=False,
        download=True, transform=dev_transformer)

    trainset_size = len(trainset)
    indices = list(range(trainset_size))
    split = int(np.floor(params.subset_percent * trainset_size))
    np.random.seed(230)
    np.random.shuffle(indices)

    train_sampler = SubsetRandomSampler(indices[:split])

    trainloader = torch.utils.data.DataLoader(trainset, batch_size=params.batch_size,
        sampler=train_sampler, num_workers=params.num_workers, pin_memory=params.cuda)

    devloader = torch.utils.data.DataLoader(devset, batch_size=params.batch_size,
        shuffle=False, num_workers=params.num_workers, pin_memory=params.cuda)

    if types == 'train':
        dl = trainloader
    else:
        dl = devloader

    return dl

import torch.nn.functional as F

class Net(nn.Module):
    """
    This is the standard way to define your own network in PyTorch. You typically choose the components
    (e.g. LSTMs, linear layers etc.) of your network in the __init__ function. You then apply these layers
    on the input step-by-step in the forward function. You can use torch.nn.functional to apply functions
    such as F.relu, F.sigmoid, F.softmax, F.max_pool2d. Be careful to ensure your dimensions are correct after each
    step. You are encouraged to have a look at the network in pytorch/nlp/model/net.py to get a better sense of how
    you can go about defining your own network.
    The documentation for all the various components available o you is here: http://pytorch.org/docs/master/nn.html
    """

    def __init__(self, params):
        """
        We define an convolutional network that predicts the sign from an image. The components
        required are:
        Args:
            params: (Params) contains num_channels
        """
        super(Net, self).__init__()
        self.num_channels = params.num_channels

        # each of the convolution layers below have the arguments (input_channels, output_channels, filter_size,
        # stride, padding). We also include batch normalisation layers that help stabilise training.
        # For more details on how to use these layers, check out the documentation.
        self.conv1 = nn.Conv2d(3, self.num_channels, 3, stride=1, padding=1)
        self.bn1 = nn.BatchNorm2d(self.num_channels)
        self.conv2 = nn.Conv2d(self.num_channels, self.num_channels*2, 3, stride=1, padding=1)
        self.bn2 = nn.BatchNorm2d(self.num_channels*2)
        self.conv3 = nn.Conv2d(self.num_channels*2, self.num_channels*4, 3, stride=1, padding=1)
        self.bn3 = nn.BatchNorm2d(self.num_channels*4)

        # 2 fully connected layers to transform the output of the convolution layers to the final output
        self.fc1 = nn.Linear(4*4*self.num_channels*4, self.num_channels*4)
        self.fcbn1 = nn.BatchNorm1d(self.num_channels*4)
        self.fc2 = nn.Linear(self.num_channels*4, 10)
        self.dropout_rate = params.dropout_rate

    def forward(self, s):
        """
        This function defines how we use the components of our network to operate on an input batch.
        Args:
            s: (Variable) contains a batch of images, of dimension batch_size x 3 x 32 x 32 .
        Returns:
            out: (Variable) dimension batch_size x 6 with the log probabilities for the labels of each image.
        Note: the dimensions after each step are provided
        """
        #                                                  -> batch_size x 3 x 32 x 32
        # we apply the convolution layers, followed by batch normalisation, maxpool and relu x 3
        s = self.bn1(self.conv1(s))                         # batch_size x num_channels x 32 x 32
        s = F.relu(F.max_pool2d(s, 2))                      # batch_size x num_channels x 16 x 16
        s = self.bn2(self.conv2(s))                         # batch_size x num_channels*2 x 16 x 16
        s = F.relu(F.max_pool2d(s, 2))                      # batch_size x num_channels*2 x 8 x 8
        s = self.bn3(self.conv3(s))                         # batch_size x num_channels*4 x 8 x 8
        s = F.relu(F.max_pool2d(s, 2))                      # batch_size x num_channels*4 x 4 x 4

        # flatten the output for each image
        s = s.view(-1, 4*4*self.num_channels*4)             # batch_size x 4*4*num_channels*4

        # apply 2 fully connected layers with dropout
        s = F.dropout(F.relu(self.fcbn1(self.fc1(s))),
            p=self.dropout_rate, training=self.training)    # batch_size x self.num_channels*4
        s = self.fc2(s)                                     # batch_size x 10

        return s


def loss_fn(outputs, labels):
    """
    Compute the cross entropy loss given outputs and labels.
    Args:
        outputs: (Variable) dimension batch_size x 6 - output of the model
        labels: (Variable) dimension batch_size, where each element is a value in [0, 1, 2, 3, 4, 5]
    Returns:
        loss (Variable): cross entropy loss for all images in the batch
    Note: you may use a standard loss function from http://pytorch.org/docs/master/nn.html#loss-functions. This example
          demonstrates how you can easily define a custom loss function.
    """
    return nn.CrossEntropyLoss()(outputs, labels)


#def loss_fn_kd(outputs, labels, teacher_outputs, params):
 #   """
  #  Compute the knowledge-distillation (KD) loss given outputs, labels.
   # "Hyperparameters": temperature and alpha
    #NOTE: the KL Divergence for PyTorch comparing the softmaxs of teacher
    #and student expects the input tensor to be log probabilities! See Issue #2
    #"""
    #alpha = params.alpha
    #T = params.temperature
    #KD_loss = nn.KLDivLoss()(F.log_softmax(outputs/T, dim=1),
    #                         F.softmax(teacher_outputs/T, dim=1)) * (alpha * T * T) + \
    #          F.cross_entropy(outputs, labels) * (1. - alpha)

    #return KD_loss


def accuracy(outputs, labels):
    """
    Compute the accuracy, given the outputs and labels for all images.
    Args:
        outputs: (np.ndarray) output of the model
        labels: (np.ndarray) [0, 1, ..., num_classes-1]
    Returns: (float) accuracy in [0,1]
    """
    outputs = np.argmax(outputs, axis=1)
    return np.sum(outputs==labels)/float(labels.size)


# maintain all metrics required in this dictionary- these are used in the training and evaluation loops
metrics = {
    'accuracy': accuracy,
    # could add more metrics such as accuracy for each token type
}

class BasicBlock(nn.Module):
    expansion = 1

    def __init__(self, in_planes, planes, stride=1):
        super(BasicBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)

        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != self.expansion*planes:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(self.expansion*planes)
            )

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)
        out = F.relu(out)
        return out

class ResNet(nn.Module):
    def __init__(self, block, num_blocks, num_classes=10):
        super(ResNet, self).__init__()
        self.in_planes = 64

        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)
        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)
        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)
        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)
        self.linear = nn.Linear(512*block.expansion, num_classes)

    def _make_layer(self, block, planes, num_blocks, stride):
        strides = [stride] + [1]*(num_blocks-1)
        layers = []
        for stride in strides:
            layers.append(block(self.in_planes, planes, stride))
            self.in_planes = planes * block.expansion
        return nn.Sequential(*layers)

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = self.layer4(out)
        out = F.avg_pool2d(out, 4)
        out = out.view(out.size(0), -1)
        out = self.linear(out)
        return out


def ResNet18():
    return ResNet(BasicBlock, [2,2,2,2])

class ResNeXtBottleneck(nn.Module):
    """
    RexNeXt bottleneck type C (https://github.com/facebookresearch/ResNeXt/blob/master/models/resnext.lua)
    """
    def __init__(self, in_channels, out_channels, stride, cardinality, widen_factor):
        """ Constructor
        Args:
            in_channels: input channel dimensionality
            out_channels: output channel dimensionality
            stride: conv stride. Replaces pooling layer.
            cardinality: num of convolution groups.
            widen_factor: factor to reduce the input dimensionality before convolution.
        """
        super(ResNeXtBottleneck, self).__init__()
        D = cardinality * out_channels // widen_factor
        self.conv_reduce = nn.Conv2d(in_channels, D, kernel_size=1, stride=1, padding=0, bias=False)
        self.bn_reduce = nn.BatchNorm2d(D)
        self.conv_conv = nn.Conv2d(D, D, kernel_size=3, stride=stride, padding=1, groups=cardinality, bias=False)
        self.bn = nn.BatchNorm2d(D)
        self.conv_expand = nn.Conv2d(D, out_channels, kernel_size=1, stride=1, padding=0, bias=False)
        self.bn_expand = nn.BatchNorm2d(out_channels)

        self.shortcut = nn.Sequential()
        if in_channels != out_channels:
            self.shortcut.add_module('shortcut_conv', nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, padding=0, bias=False))
            self.shortcut.add_module('shortcut_bn', nn.BatchNorm2d(out_channels))

    def forward(self, x):
        bottleneck = self.conv_reduce.forward(x)
        bottleneck = F.relu(self.bn_reduce.forward(bottleneck), inplace=True)
        bottleneck = self.conv_conv.forward(bottleneck)
        bottleneck = F.relu(self.bn.forward(bottleneck), inplace=True)
        bottleneck = self.conv_expand.forward(bottleneck)
        bottleneck = self.bn_expand.forward(bottleneck)
        residual = self.shortcut.forward(x)
        return F.relu(residual + bottleneck, inplace=True)

class CifarResNeXt(nn.Module):
    """
    ResNext optimized for the Cifar dataset, as specified in
    https://arxiv.org/pdf/1611.05431.pdf
    """
    def __init__(self, cardinality, depth, num_classes, widen_factor=4, dropRate=0):
        """ Constructor
        Args:
            cardinality: number of convolution groups.
            depth: number of layers.
            num_classes: number of classes
            widen_factor: factor to adjust the channel dimensionality
        """
        super(CifarResNeXt, self).__init__()
        self.cardinality = cardinality
        self.depth = depth
        self.block_depth = (self.depth - 2) // 9
        self.widen_factor = widen_factor
        self.num_classes = num_classes
        self.output_size = 64
        self.stages = [64, 64 * self.widen_factor, 128 * self.widen_factor, 256 * self.widen_factor]

        self.conv_1_3x3 = nn.Conv2d(3, 64, 3, 1, 1, bias=False)
        self.bn_1 = nn.BatchNorm2d(64)
        self.stage_1 = self.block('stage_1', self.stages[0], self.stages[1], 1)
        self.stage_2 = self.block('stage_2', self.stages[1], self.stages[2], 2)
        self.stage_3 = self.block('stage_3', self.stages[2], self.stages[3], 2)
        self.classifier = nn.Linear(1024, num_classes)
        init.kaiming_normal(self.classifier.weight)

        for key in self.state_dict():
            if key.split('.')[-1] == 'weight':
                if 'conv' in key:
                    init.kaiming_normal(self.state_dict()[key], mode='fan_out')
                if 'bn' in key:
                    self.state_dict()[key][...] = 1
            elif key.split('.')[-1] == 'bias':
                self.state_dict()[key][...] = 0

    def block(self, name, in_channels, out_channels, pool_stride=2):
        """ Stack n bottleneck modules where n is inferred from the depth of the network.
        Args:
            name: string name of the current block.
            in_channels: number of input channels
            out_channels: number of output channels
            pool_stride: factor to reduce the spatial dimensionality in the first bottleneck of the block.
        Returns: a Module consisting of n sequential bottlenecks.
        """
        block = nn.Sequential()
        for bottleneck in range(self.block_depth):
            name_ = '%s_bottleneck_%d' % (name, bottleneck)
            if bottleneck == 0:
                block.add_module(name_, ResNeXtBottleneck(in_channels, out_channels, pool_stride, self.cardinality,
                                                          self.widen_factor))
            else:
                block.add_module(name_,
                                 ResNeXtBottleneck(out_channels, out_channels, 1, self.cardinality, self.widen_factor))
        return block

    def forward(self, x):
        x = self.conv_1_3x3.forward(x)
        x = F.relu(self.bn_1.forward(x), inplace=True)
        x = self.stage_1.forward(x)
        x = self.stage_2.forward(x)
        x = self.stage_3.forward(x)
        x = F.avg_pool2d(x, 8, 1)
        x = x.view(-1, 1024)
        return self.classifier(x)

base_data ={
    "model_version": "resnet18",
    "subset_percent":1.0,
    "augmentation": "yes",
    "teacher": "none",
    "alpha": 0.0,
    "temperature": 1,
    "learning_rate": 1e-1,
    "batch_size": 128,
    "num_epochs": 175,
    "dropout_rate": 0.5,
    "num_channels": 64,
    "save_summary_steps": 100,
    "num_workers": 4
}
params = DotDict(base_data)

# use GPU if available
params.cuda = torch.cuda.is_available()
# Set the random seed for reproducible experiments
random.seed(230)
torch.manual_seed(230)
if params.cuda: torch.cuda.manual_seed(230)

params.batch_size = 100

#train_dl = fetch_dataloader('train', params)
#val_dl = fetch_dataloader('dev', params)

#!mkdir resnet18_minimaltest

model_dir = '/bin/smartinez/resnet18minimaltest'


params.cuda = torch.cuda.is_available()

# Set the random seed for reproducible experiments
random.seed(230)
torch.manual_seed(230)
if params.cuda: torch.cuda.manual_seed(230)

tb = SummaryWriter()
#mages, labels = next(iter(train_dl))
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

#images, labels = images.to(device), labels.to(device)
#grid = torchvision.utils.make_grid(images)

#tb.add_image('images', grid)
#tb.add_graph(student_model, images)
#tb.close()

"""**Student model (Resnet18) without any training**"""

#evaluate(student_model, loss_fn, val_dl, metrics, params)

"""**Student model loading the best weights from training (No KD)**"""

#base_student_checkpoint = '/content/drive/MyDrive/KD/experiments/base_resnet18/best.pth.tar'
#load_checkpoint(base_student_checkpoint, student_model, optimizer);

#evaluate(student_model, loss_fn, val_dl, metrics, params)

params.cuda = torch.cuda.is_available()

# Set the random seed for reproducible experiments
random.seed(230)
torch.manual_seed(230)
if params.cuda: torch.cuda.manual_seed(230)

"""**Student model with bes tweights after KD with resnext29 as teacher**"""

#kd_student_checkpoint = '/content/drive/MyDrive/KD/experiments/resnet18_distill/resnext_teacher/best.pth.tar'
#load_checkpoint(kd_student_checkpoint, student_model, optimizer);

#evaluate(student_model, loss_fn, val_dl, metrics, params)

"""**RESULTS INDICATE BETTER ACCURACY AFTER KD BUT LOSE IS A LITTLE BIT BIGGER AFTER KD**"""

#train_and_evaluate(student_model, train_dl, val_dl, optimizer, loss_fn, metrics, params,
#                           model_dir, restore_file=None)
# fetch loss function and metrics definition in model files
#loss_fn_kd = loss_fn_kd
#metrics=metrics

#evaluate(student_model, loss_fn, val_dl, metrics, params)


#evaluate(teacher_model, loss_fn, val_dl, metrics, params)

params.cuda

kd_data = {
    "model_version": "resnet18_distill",
    "subset_percent": 1.0,
    "augmentation": "yes",
    "teacher": "resnext",
    "alpha": 0.95,
    "temperature": 6,
    "learning_rate": 1e-1,
    "batch_size": 128,
    "num_epochs": 175,
    "dropout_rate": 0.0,
    "num_channels": 32,
    "save_summary_steps": 100,
    "num_workers":4
}
params = DotDict(kd_data)

params.batch_size =100

class SaveFeatures():
    features=None
    def __init__(self, m): self.hook = m.register_forward_hook(self.hook_fn)  # attach the hook to the specified layer
    def hook_fn(self, module, input, output): self.features = output.data # copy the activation features as an instance variable
    def remove(self): self.hook.remove()

def check_int(variable):
    if not isinstance(variable, int):
        #print("Error, index is not integer")
        # You can raise an exception here to stop the execution if needed.
        # raise ValueError("Error, index is not integer")
        return True

def print_if_all_zero(array_or_tensor):
    if isinstance(array_or_tensor, np.ndarray):
        array_or_tensor = torch.tensor(array_or_tensor)

    if torch.all(array_or_tensor == 0):
        print(array_or_tensor)
        return True

def getCAM(feature_conv, weight_fc, class_idx):
    feature_conv = feature_conv.squeeze()
    #_, nc, h, w = feature_conv.shape
    #print("feature_conv "+str(feature_conv))
    nc, h, w = feature_conv.shape
    if (check_int(class_idx)):
      class_idx=class_idx.item()

      #print("class_index isn't integer, look at this: "+str(class_idx))
    #print("class index: "+str(class_idx))
    #print("number of channels: "+str(nc))
    #print("height: "+str(h))
    #print("width: "+str(w))
    #print("h*d: "+str(h*w))
    #print("shape of the softmax weights: "+str(weight_fc.shape))
    #print("shape of the last convolution layer of the model in question: "+str(feature_conv.shape))
    #print("shape of dot product of the softmax weights and the reshaped last layer: "+str(weight_fc.dot(feature_conv.reshape((nc, h*w))).shape))
  #  cam = weight_fc[class_idx].dot(feature_conv.reshape((nc, h*w)))
   # print("weight_fc[class_idx].T shape "+str(weight_fc[class_idx].T.shape ))
    #print("weight_fc[class_idx] shape "+str( weight_fc[class_idx].shape))
    cam = (weight_fc[class_idx] @ feature_conv.reshape((nc, h*w)))
    #print("type of feature conv "+str(type(feature_conv)))
    #print("gradient of feature conv "+str(feature_conv.requires_grad))
    #print("cam type "+str(type(cam)))
    #print("cam gradient "+str(cam.requires_grad))
    if(print_if_all_zero(cam)):
      print("weight_fc[class_idx]: "+str(weight_fc[class_idx]))
      #print("feature_conv: "+str(feature_conv))
    if (check_nan(cam)):
      print("Nan detected on cam before reshaping")

    #print("reshaped last layer activations (nc, h*w): "+str(cam.shape))
    cam = cam.reshape(h, w)
    if(check_nan(cam)):
      print("NaN detected on cam AFTER reshaping")
    cam = cam - torch.min(cam)
    if(check_nan(cam)):
      print("NaN detected on cam AFTER cam - np.min(cam)")

    cam_img = cam / torch.max(cam)
   # print("cam img "+str(cam_img))
    #print("cam img shape "+str(cam_img.shape))
    if(check_nan(cam_img)):
      print("cam: "+str(cam))
      print("NaN detected on cam_img, see the value of np.max(cam) below (maybe it has a NaN, who knows?):")
     # print(np.max(cam))
    #  print("cam after the dot product thing: "+str(weight_fc[class_idx].dot(feature_conv.reshape((nc, h*w)))))
     # print("cam after reshaping: "+str(weight_fc[class_idx].dot(feature_conv.reshape((nc, h*w))).reshape(h,w)))
      print("class index: "+str(class_idx))
      print("number of channels: "+str(nc))
      print("height: "+str(h))
      print("width: "+str(w))
      print("h*d: "+str(h*w))
      exit(420)
    
    return cam_img

#overlay = getCAM(activated_features.features, weight_softmax, 0. )
#get a heatmap for every class 
def getclassCAMS(feature_conv, weight_fc):
    #classcams = []
    #initcam = (weight_fc[1].T @ reshaped_feats).reshape(h, w)
    #initcam = initcam - torch.min(initcam)
        #classcams.append(minned_cam / torch.max(minned_cam))
    #initcam = initcam / torch.max(initcam)
    initcam = getCAM(feature_conv, weight_fc, 1)
    print("initcam shape [0] "+str(initcam.shape[0]))
    result_tensor = torch.empty((10, initcam.shape[0], initcam.shape[0]), dtype=torch.float64)
    #result_tensor = torch.empty(10, dtype=torch.float64)
    feature_conv = feature_conv.squeeze()
    nc, h, w = feature_conv.shape
    #print("thingy: "+str(_))
    #print("features shape: "+str(feature_conv.shape))
    #print("nc: "+str(nc))
    #print("h: "+str(h))
    #print("w:"+str(w))
    reshaped_feats = feature_conv.reshape((nc, h*w))
    #print("reshaped feats: "+str(reshaped_feats.shape))
    for class_idx in range(10):
    #    cam = weight_fc[class_idx].dot(reshaped_feats).reshape(h, w)
       # cam = torch.mm(weight_fc[class_idx]).reshape(h, w)
       # cam = (weight_fc[class_idx].T @ reshaped_feats).reshape(h, w)
        cam = (weight_fc[class_idx] @ reshaped_feats).reshape(h, w)
        minned_cam = cam - torch.min(cam)
        #classcams.append(minned_cam / torch.max(minned_cam))
        result_tensor[class_idx] = minned_cam / torch.max(minned_cam)
    return (result_tensor)

#get a batch of heatmaps for all classes
def classCAMSbatch (feature_conv, weight_fc):
  initcam = getCAM(feature_conv, weight_fc, 1)
  if len(feature_conv)>1 :
    #heatmap_batch =[]
    print("initcam shape [0] "+str(initcam.shape[0]))
    result_tensor = torch.empty((len(feature_conv), initcam.shape[0], initcam.shape[0]), dtype=torch.float64)
   ##print("length of batch: "+str(len(feature_conv)))
    for (index) in range(len(feature_conv)):
      currentcams = getclassCAMS(feature_conv[index], weight_fc)
      result_tensor[index] = currentcams
    return result_tensor
  elif len(feature_conv)==1:
    print("number of samples in batch is exactly 1")
    return getclassCAMS(feature_conv, weight_fc)
  else:
    print("number of samples is batch is NOT 1 nor more than one so something must be really wrong")

def getCAMBatch (feature_conv, weight_fc, labels_batch):
  initcam = getCAM(feature_conv[1], weight_fc, 1)
  
  if len(feature_conv)>1 :
   # heatmap_batch =[]
    
    result_tensor = torch.empty((len(feature_conv), initcam.shape[0], initcam.shape[0]), dtype=torch.float64)
    #print("lenght of feature conv"+str(len(feature_conv)))
    for (index) in range(len(feature_conv)):
      currentcam = getCAM(feature_conv[index], weight_fc, labels_batch[index])
   #   print("current cam shape "+str(currentcam.shape))
     # heatmap_batch.append(currentcam)
      #print("result tensor "+str(result_tensor))
    #  print("result tensor shape "+str(result_tensor.shape))
      result_tensor[index] = currentcam
    #print("heatmap_batch shape "+str(heatmap_batch.shape))
    #heatmap_batch = np.array(heatmap_batch)
    return result_tensor
  elif len(feature_conv)==1:
    print("number of samples in batch is exactly 1")
    return getclassCAMS(feature_conv, weight_fc)
  else:
    print("number of samples is batch is NOT 1 nor more than one so something must be really wrong")

params.cuda = torch.cuda.is_available()

# Set the random seed for reproducible experiments
random.seed(230)
torch.manual_seed(230)
if params.cuda: torch.cuda.manual_seed(230)

#params.batch_size

"""TEST Train and evaluate KD"""


#thresholdrun = wandb.init(
    # Set the project where this run will be logged
 #   project="thresholdsweep",
    # Track hyperparameters and run metadata
  # sweep_config = {
   # 'method': 'bayes'
    #sweepmetric = {
     #   'name': 'loss',
      #  'goal': 'minimize'   
    #}})
"""" sweep_config = {
    'method': 'bayes'
    }
sweepmetric = {
    'name': 'val_acc',
    'goal': 'maximize'   
    }

sweep_config['metric'] = sweepmetric
parameters_dict = {
    'KLDgamma': {
        'values': [0.3, 0.5, 0.75, 1]
        },
    'threshold': {
        'values': [0.3, 0.5, 0.75]
        },
    'experiment': {
          'values': ['TrueClass', 'TopClass', 'AllClasses']
        }
    }

sweep_config['parameters'] = parameters_dict

#threshsweep_id = wandb.sweep(sweep_config, project="thresholdsweep")
#
#"""


def modelrun(model, teacher_model, train_dataloader, val_dataloader, optimizer,
                loss_fn, metrics, params, threshold,student_arch, teacher_arch, usethresh= False, useKD = True, restore_file=None,experiment='TrueClass',
                KLDgamma = 1, wandbrun= False):
    if useKD == False:
        print("doing vanilla run without KD")   
        train_and_evaluate(model, train_dataloader, val_dataloader, optimizer,
                       loss_fn, metrics, params, student_arch, restore_file, wandbrun)
        
    if usethresh == True:
        print("doing run with threshold KD")
        train_and_evaluate_kd_thresh(model, teacher_model, train_dataloader, val_dataloader, optimizer,
                loss_fn_kd_thresh, metrics, params, threshold, student_arch, teacher_arch, restore_file,experiment,KLDgamma, wandbrun)
    elif usethresh == False:
        print("doing KD run without threshold")
        
        
def nothreshsweeprun(config=None):
  with wandb.init(config=config):
       # print("line before config")
        config = wandb.config
        #print("line before dotdict")
        params = DotDict(kd_data)
        #print("line before params.cuda")
        params.cuda = torch.cuda.is_available()
        params.num_epochs = 40

# Set the random seed for reproducible experiments
        random.seed(230)
        torch.manual_seed(230)
        if params.cuda: torch.cuda.manual_seed(230)
        student_model = ResNet18().cuda() if params.cuda else ResNet18()
        print("student model initialised from scratch")
        train_dl = fetch_dataloader('train', params)
        val_dl = fetch_dataloader('dev', params)
        #train_dl = fetch_dataloader('train', params)
       # val_dl = fetch_dataloader('dev', params)
        #print("initialising teacher model")
        teacher_model = CifarResNeXt(cardinality=8, depth=29, num_classes=10)
        teacher_checkpoint = '/home/smartinez/experiments/base_resnext29/best.pth.tar'
        teacher_model = nn.DataParallel(teacher_model).cuda()
        #print("teacher model loaded")
        teacher_model.to(device);
        print("teacher model loaded to gpu")
        load_checkpoint(teacher_checkpoint, teacher_model);
        print("loaded teacher weights")
        optimizer = optim.SGD(student_model.parameters(), lr=params.learning_rate,
                      momentum=0.9, weight_decay=5e-4)
        print("created optimiser")
        #model_dir = '/bin/smartinez/resnet18minimaltest'
        restore_file = None
        
        metrics = {
    'accuracy': accuracy
    # could add more metrics such as accuracy for each token type
}       
       # print("line before calling train and evaluate")
        #print("beginning train and evaluate loop")
        train_and_evaluate_kd_nothresh(student_model, teacher_model, train_dl, val_dl, optimizer,
                       loss_fn_kd_nothresh, metrics, params, "resnet18", "resnext29",  restore_file,experiment = config.experiment, KLDgamma= config.KLDgamma, wandbrun= True)
#        train_and_evaluate_kd_nothresh(student_model, teacher_model, train_dl, val_dl, optimizer,
 #                      loss_fn_kd_nothresh, metrics, params, restore_file, experiment =config.experiment, KLDgamma = config.KLDgamma, wandbrun = True )


res18kdparams ={
    "model_version": "resnet18_distill",
    "subset_percent": 1.0,
    "augmentation": "yes",
    "teacher": "resnext29",
    "alpha": 0.95,
    "temperature": 6,
    "learning_rate": 1e-1,
    "batch_size": 128,
    "num_epochs": 175,
    "dropout_rate": 0.0, 
    "num_channels": 32,
    "save_summary_steps": 100,
    "num_workers": 4
}
params.num_epochs = 40
params = DotDict(res18kdparams)

sweep_configuration_nothresh = {
    'method': 'grid',
    #'name': 'threshsweeperino',
    'metric': {
        'goal': 'maximize', 
        'name': 'val_acc'
        },
    "num_epochs": params.num_epochs,
    "model": params.model_version,
    "teacher": params.teacher,
    "batch_size": params.batch_size  ,
    "UseThresh": False,  
    'parameters': {
        #'KLDgamma': {'values': [0.3, 0.5, 0.75, 1.0]},
        #'KLDgamma': {'values': [0.3, 0.25, 0.2, 0.15]},#
        'KLDgamma': {'values': [0, 0.25, 0.5, 0.75]},#
        #'threshold': {'values': [0.3, 0.4, 0.5, 0.75]}
        'experiment': {'values': ['TrueClass','TopClass','AllClasses']}
     
     }
}



nothreshsweep_id =wandb.sweep(sweep_configuration_nothresh, project="tensorlossruns")
print("performing no threshold hyperparameter search")
wandb.agent(nothreshsweep_id, nothreshsweeprun) 

#try:
 #   wandb.agent(nothreshsweep_id, nothreshsweeprun)   
#except Exception as e:
    # exit gracefully, so wandb logs the problem
#  print(traceback.format_exc())
 # exit(1)
     
sweep_configuration_tresh = {
    'method': 'grid',
    #'name': 'threshsweeperino',
    'metric': {
        'goal': 'maximize', 
        'name': 'val_acc'
        },
    "num_epochs": params.num_epochs,
    "model": params.model_version,
    "teacher": params.teacher,
    "batch_size": params.batch_size  ,
    "UseThresh": True,  
    'parameters': {
        #'KLDgamma': {'values': [0.3, 0.5, 0.75, 1.0]},
       'KLDgamma': {'values': [ 0, 0.25, 0.5, 0.75, 1]},#
        'threshold': {'values': [0.3, 0.5, 0.7]},
        'experiment': {'values': ['TrueClass','TopClass','AllClasses']}
     
     }
}        
        
        
        
def threshsweeprun(config=None):
  with wandb.init(config=config):
       # print("line before config")
        config = wandb.config
        #print("line before dotdict")
        params = DotDict(kd_data)
        #print("line before params.cuda")
        params.cuda = torch.cuda.is_available()
        params.num_epochs = 50

# Set the random seed for reproducible experiments
        random.seed(230)
        torch.manual_seed(230)
        if params.cuda: torch.cuda.manual_seed(230)
        student_model = ResNet18().cuda() if params.cuda else ResNet18()
        print("Student model created from scratch")
        train_dl = fetch_dataloader('train', params)
        val_dl = fetch_dataloader('dev', params)
        teacher_model = CifarResNeXt(cardinality=8, depth=29, num_classes=10)
        teacher_checkpoint = '/home/smartinez/experiments/base_resnext29/best.pth.tar'
        teacher_model = nn.DataParallel(teacher_model).cuda()
        teacher_model.to(device);
        load_checkpoint(teacher_checkpoint, teacher_model);
        optimizer = optim.SGD(student_model.parameters(), lr=params.learning_rate,
                      momentum=0.9, weight_decay=5e-4)
        #model_dir = '/bin/smartinez/resnet18minimaltest'
        restore_file = None
        #experiment = 'AllClasses'
        #KLDgamma = 0.3
        
        metrics = {
    'accuracy': accuracy
    # could add more metrics such as accuracy for each token type
}       
        train_and_evaluate_kd_thresh(student_model, teacher_model, train_dl, val_dl, optimizer,
                                     loss_fn_kd_thresh, metrics, params, config.threshold, "resnet18", "resnext29", restore_file, config.experiment, config.KLDgamma, wandbrun= True)
       # print("line before calling train and evaluate")
       # train_and_evaluate_kd_thresh(student_model, teacher_model, train_dl, val_dl, optimizer,
        #              loss_fn_kd_thresh, metrics, params, model_dir, config.threshold, restore_file, config.experiment, config.KLDgamma, wandbrun = True )
        
#wandb.agent(threshsweep_id, threshsweeprun, count=20)        
        
#student_model = ResNet18().cuda() if params.cuda else ResNet18()


#optimizer = optim.SGD(student_model.parameters(), lr=params.learning_rate,
 #                momentum=0.9, weight_decay=5e-4)
#print("parameters" +str(params))

#train_dl = fetch_dataloader('train', params)
#val_dl = fetch_dataloader('dev', params)

threshsweep_id =wandb.sweep(sweep_configuration_tresh, project="tensorlossruns")
print("performing threshold hyperparameter search")

wandb.agent(threshsweep_id, threshsweeprun)  
#try:
 #  wandb.agent(threshsweep_id, threshsweeprun)   
#except Exception as e:
    # exit gracefully, so wandb logs the problem
 #   print(traceback.format_exc())
  #  exit(1)
# start a new wandb run to track this script


def fullkdtrainrun(projectname, config=None ):
  with wandb.init(config=config,resume=False, project = projectname):
       # print("line before config")
        config = wandb.config
        #print("line before dotdict")
        params = DotDict(kd_data)
        #print("line before params.cuda")
        params.cuda = torch.cuda.is_available()

# Set the random seed for reproducible experiments
        random.seed(230)
        torch.manual_seed(230)
        if params.cuda: torch.cuda.manual_seed(230)
        student_model = ResNet18().cuda() if params.cuda else ResNet18()
        print("student model initialised from scratch")
        train_dl = fetch_dataloader('train', params)
        val_dl = fetch_dataloader('dev', params)
        #train_dl = fetch_dataloader('train', params)
       # val_dl = fetch_dataloader('dev', params)
        teacher_model = CifarResNeXt(cardinality=8, depth=29, num_classes=10)
        teacher_checkpoint = '/bin/smartinez/best.pth.tar'
        teacher_model = nn.DataParallel(teacher_model).cuda()
        teacher_model.to(device);
        load_checkpoint(teacher_checkpoint, teacher_model);
        optimizer = optim.SGD(student_model.parameters(), lr=params.learning_rate,
                      momentum=0.9, weight_decay=5e-4)
       # model_dir = '/bin/smartinez/resnet18noheatmap'
        restore_file = None
        #experiment = 'AllClasses'
        #KLDgamma = 0.3 #0.3 is the best but 1.0 is for the control experiment
        
        metrics = {
    'accuracy': accuracy
    # could add more metrics such as accuracy for each token type
}       
       # print("line before calling train and evaluate")
        train_and_evaluate_kd_nothresh(student_model, teacher_model, train_dl, val_dl, optimizer,
                       loss_fn_kd_nothresh, metrics, params, model_dir, restore_file, experiment = config.experiment, KLDgamma = config.KLDgamma, wandbrun = True )


    # 

"""config={
"KLDgamma": 1.0,
"student": "Resnet18",
"teacher": "Resnext29",
"experiment": "AllClasses"
}
   """
   
   

def fullbasetrainrun(projectname, config=None ):
  with wandb.init(config=config,resume=False, project = projectname):
       # print("line before config")
        config = wandb.config
        #print("line before dotdict")
        #params = DotDict(kd_data)
        #print("line before params.cuda")
        params.cuda = torch.cuda.is_available()

# Set the random seed for reproducible experiments
        random.seed(230)
        torch.manual_seed(230)
        if params.cuda: torch.cuda.manual_seed(230)
        student_model = ResNet18().cuda() if params.cuda else ResNet18()
        print("student model initialised from scratch")
        train_dl = fetch_dataloader('train', params)
        val_dl = fetch_dataloader('dev', params)
        #train_dl = fetch_dataloader('train', params)
       # val_dl = fetch_dataloader('dev', params)
        teacher_model = CifarResNeXt(cardinality=8, depth=29, num_classes=10)
        teacher_checkpoint = '/bin/smartinez/best.pth.tar'
        teacher_model = nn.DataParallel(teacher_model).cuda()
        teacher_model.to(device);
        load_checkpoint(teacher_checkpoint, teacher_model);
        optimizer = optim.SGD(student_model.parameters(), lr=params.learning_rate,
                      momentum=0.9, weight_decay=5e-4)
        model_dir = '/bin/smartinez/resnet18basetest'
        restore_file = None
        #experiment = 'AllClasses'
        #KLDgamma = 0.3 #0.3 is the best but 1.0 is for the control experiment
        
        metrics = {
    'accuracy': accuracy
    # could add more metrics such as accuracy for each token type
}       
        train_and_evaluate(student_model, train_dl, val_dl, optimizer, loss_fn, metrics, params,
                           model_dir, restore_file, wandbrun = True) 
       # print("line before calling train and evaluate")
        

res18baseparams = {
    "model_version": "resnet18",
    "subset_percent": 1.0,
    "augmentation": "yes",
    "teacher": "none",
    "alpha": 0.0,
    "temperature": 1,
    "learning_rate": 1e-1,
    "batch_size": 128,
    "num_epochs": 45, #was 200
    "dropout_rate": 0.5, 
    "num_channels": 32,
    "save_summary_steps": 100,
    "num_workers": 4
}
params = DotDict(res18baseparams)
   
res18baseconfig ={
"num_epochs": params.num_epochs,
"model": params.model_version,
"teacher": params.teacher,
"batch_size": params.batch_size,
"UseThresh": False      
    } 

projectname = "base vanilla train runs"
fullbasetrainrun("base vanilla train runs", res18baseconfig)  
#fulltrainrun("fulltrainruns", config)     
#teacher_model = CifarResNeXt(cardinality=8, depth=29, num_classes=10)
#teacher_checkpoint = 'experiments/base_resnext29/best.pth.tar'
#teacher_checkpoint = '/bin/smartinez/best.pth.tar'
#teacher_model = nn.DataParallel(teacher_model).cuda()
#teacher_model.to(device);
#print("Finished base vanillla training of resnet18")
res18kdparams ={
    "model_version": "resnet18_distill",
    "subset_percent": 1.0,
    "augmentation": "yes",
    "teacher": "resnext29",
    "alpha": 0.95,
    "temperature": 6,
    "learning_rate": 1e-1,
    "batch_size": 128,
    "num_epochs": 175,
    "dropout_rate": 0.0, 
    "num_channels": 32,
    "save_summary_steps": 100,
    "num_workers": 4
}
params = DotDict(res18kdparams)
res18kdnoheatconfig ={
"num_epochs": params.num_epochs,
"model": params.model_version,
"teacher": params.teacher,
"batch_size": params.batch_size,
"KLDgamma": 1.0,
"experiment": "AllClasses"     
    } 
#print("proceeding with base KD training of resnet18 with resnext29 as teacher (no heatmaps)")

#fullkdtrainrun("fulltrainruns", res18kdnoheatconfig)  
#print("finished base KD training of resnet18 with resnext29 as teacher (no heatmaps)")

res18kdheatconfig ={
"num_epochs": params.num_epochs,
"model": params.model_version,
"teacher": params.teacher,
"batch_size": params.batch_size,
"KLDgamma": 0.3,
"experiment": "AllClasses"     
    } 
#print("proceeding with KD training run of resnet18 with resnext29 as teacher WITH heatmaps")
#fullkdtrainrun("fulltrainruns", res18kdheatconfig)  
cnn_params={
    "model_version": "cnn",
    "subset_percent": 1.0,
    "augmentation": "no",
    "teacher": "none",
    "alpha": 0,
    "temperature": 1,
    "learning_rate": 1e-1,
    "batch_size": 128,
    "num_epochs": 30,
    "dropout_rate": 0.5, 
    "num_channels": 32,
    "save_summary_steps": 100,
    "num_workers": 4
}

params = DotDict(cnn_params)

cnn_base_config ={
"num_epochs": params.num_epochs,
"model": params.model_version,
"teacher": params.teacher,
"batch_size": params.batch_size      
    } 

"""Literature research part.
Cynthia rudin paper on explanations. SCS paper, research project. Pros and cons and why are they helpful or not.
Knowledge distillation in theory (cite papers).
Explanation methods, specifically post hoc methods , namely Grad-CAM
General knowledge distillation concept.
Methodology
"""


#load_checkpoint(teacher_checkpoint, teacher_model);

#train_and_evaluate_kd_thresh(student_model, teacher_model, train_dl, val_dl, optimizer,
 #                      loss_fn_kd_thresh, metrics, params, model_dir, threshold=0.3, restore_file=None, experiment= 'AllClasses', KLDgamma= 0.3,sweeprun = False )


#train_and_evaluate_kd_thresh(student_model, teacher_model, train_dl, val_dl, optimizer,
 #                   loss_fn_kd_thresh, metrics, params, model_dir, threshold=0.25, restore_file=None, experiment = 'AllClasses', KLDgamma=1)

#train_and_evaluate_kd_nothresh(student_model, teacher_model, train_dl, val_dl, optimizer, loss_fn_kd_nothresh,
#                            metrics, params, model_dir, restore_file=None, experiment = 'AllClasses',KLDgamma=0.3)

#model, teacher_model, train_dataloader, val_dataloader, optimizer,
#                       loss_fn_kd_thresh, metrics, params, model_dir, threshold, restore_file,experiment, KLDgamma):

#train_and_evaluate_kd(student_model, teacher_model, train_dl, val_dl, optimizer, loss_fn_kd,
#                             metrics, params, model_dir, restore_file=None, experiment = 'NoHeatmap')


#train_and_evaluate_kd(student_model, teacher_model, train_dl, val_dl, optimizer, loss_fn_kd,
#                             metrics, params, model_dir, restore_file=None, experiment = 'TrueClass')



#evaluate(student_model, loss_fn, val_dl, metrics, params)

#evaluate(teacher_model, loss_fn, val_dl, metrics, params)

#student_model


"""1st  experiment: teacher predicts and genrates heatmap of highest score class.
Then take student and generate heatmap of that class of the same image

2nd experiment: generate heamtap of the correct class both by student and by teacher (doesn't make so much sense because if the teacher is bad at it then the student learns to be bad at it so this would probably perform worse)

3rd experiment: generate heatmap for every class both by student and teacher

2 experiment types: do the 3 experiments with the threshold for the heatmaps

and do the 3 again without the thershold for the heatmaps
create weights (for example alpha for the kulback leibler and beta for the heatmap loss)

  
"""
